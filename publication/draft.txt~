# Deep convolutional feature random forest (DCFRF) classification for image segmentation

## Authors

- Christian Tischer
- Ignacio Arganda-Carreras
- Anna Steyer
- Yannick Schwab
- Rainer Pepperkok

## Figures

## Abstract

##
## Introduction
##

- The quantification and visualization of the content of images often involves the segmentation of certain structures of interest. In fluorescence microscopy images such segmentation can sometimes be achieved using a simple image processing workflow such as local background subtraction followed by global thresholding. In many cases however, especially in electron microscopy images, such simple workflows do not suffice and more complex processing protocols are required. The development of such adavanced processing protocols can easily take many days, without the guarantee of success, such that electron microscopy are currently often still segmented fully manually. While guaranteed to succeed, manual segmentation is very cumbersome and can require hours to days. For example, segmentation of the endoplasmic reticulum covering the DNA in anaphase cells took ?? hours (@Anna: how long?). Thus, machine learning approaches that automatically learn segmentation rules from sparse annotations are of great interest, because a human needs to segment only a subset of the data set (the annotations) while the remainder is automatically segmented by the machine.  

- Current user friendly machine learning solutions comprise the segmentation toolkit ilastik [Ref_ilastik] and the Trainable Weka Segmentation (TWS) Fiji plugin [Ref_TWS] (@Anna: Shall we mentioned something else here?). Both tools compute for each pixel a feature vector, comprising the values of several image features such as, e.g., Gaussian blurring or Hessian matrix eigenvalues. Subsequently, a random forest algorithm [Ref_Breiman] is trained to segment image pixels based on their feature vectors into different structures (classes). While this approach is very powerful and has proven useful in several occasions (@All: Examples?) deep convolutional neural networks (DCNN) seem even more promising in terms of image segmentation capabilities. DCNNs have achieved high ranking results in segmentation challenges and, in fact, seem conceptually more powerful because of the deep convolution capabilities which are currently lacking in ilastik and TWS. However, as far as we know there currently is no user friendly DCNN implementation that could be used by scientist without programming experience. In addition, given perfectly annotated data, the training of a typical DCNN currently takes hours to days [Ref_3DUNet], making it hard to use 
  
- We combine the power of deep convolution with the speed of a random forest classifier
- We present the segmentation of several challenging EM data sets, showing that our approach is
	- interactive
	- accurate
	- fast
	- big data compatible

##
## Implementation
##

The DCFRF segmentation tool is implemented as a Fiji plugin and can be installed via Fiji's update manager. The DCFRF plugin is based on the Trainable Weka Segmentation (TWS) plugin [Ref_TWS]. In the following we will outline the main differences between the DCFRF and the TWS plugin.

### Storage of labels

While in the TWS only the training data associated with each label was stored, now also the actual ROI is stored. This enables the user to come back to the same data set and add more labels, seeing which labels have been put previously.

### Feature computation

Computing features at higher resolution levels is not done by increasing the kernel width but by down-sampling the input image. This has the advantage of an increased speed during feature computation, as well as reduced memory requirements for storing the feature images. For N-D data with a binning factor of B, the reduction in computation time and memory is a factor of B^N for each resolution layer. For classification, the down-sampled feature images are up-sampled again (just as in the 3-D U-Net), this takes time such that some of the gain in speed is lost; we would like to explore in the future whether this up-sampling could be computed on a GPU in order to save time. As the up-sampling is only needed locally at the location of the current instance voxel, the additional memory requirements at this step are relatively small.

In addition, features at lower resolution layers are not only computed on the down-sampled original image but also on down-sampled feature images of the previous resolution layer (see section on deep convolutional feature computation). 

### N-D support

The DCFRF plugin now supports multi-channel and multi-time-point data. In terms of multi-channel support the user can choose which channels should be taken into account for the feature computation. Fatures are computed in all channels independenly; we currently do not compute features, which combine gray values from multiple channels.

### Big image data handling

The DCFRF plugin processes the image data in blocks and can thus handle arbitrarily large images. For both the input as well as the classification image the user has the choice to have either of them fully in RAM or stream the data from and to disk. The streaming is currently handled  In addition the user can specify subregions of the image to be classified such that not always the whole data set needs to be processed.

### Uncertainty display and navigation

As is possible in ilastik one can activate an uncertainty overlay, showing the classification margin, i.e. the difference between the most and second likely class probabilities. This helps the user to see where more labelling is needed. However, while this is very useful, we found that in a large (4-D) data set it takes too much time to manually navigate through the data set searching for such regions. We thus implemented an "uncertainty navigation". During classification we keep track of the average uncertainty in each classified image block and store this information in a sorted list. Using keyboard shortcuts the user can navigate through this list while the region is highlighted on the input image. Like this, informative additional labels can be added very efficiently. 

### Feature importance and subsetting

The classical way to compute feature importances in RFs is to run an out-of-bag sample of the training data through the RF, and compare the classification results when one feature is exchanged by a random other feature [Ref Breiman]. As we typically have aroun2000 features and 400 trees, executing above recipe for all features takes a considerable amount of time and would thus perturb the interactivity of our tool. We thus opted for a different option: During the training we simply count how often each feature was used in the whole forest. The idea being that features which have been selected only at few nodes (in few trees) are probably not very important for the overall classification outcome. Such rarely used features can be deactived. In our current implementation this mainly speeds up the feature upsampling as this now needs to be done for less features. Currently we still compute all features (also deactivated) because features in later resolution levels are derived from features in earlier resolution levels such that it becomes somewhat involved to figure out which features can be left out during the feature computation stage. Moreover, after deactivating rarely used features we run the RF training once more, only taking into account the active features. Here our intuition is that the RF can learn more informative relations between actually useful features (TODO: test this somehow).

   
## Deep convolutional feature computation

The advantage of a neural network implementation of deep convolution is that one does not have to manually choose which convolutions are computed, because this is learned during the training. However this results in many parameters to be learned and thus long training times (typically hours). Here we chose to use fixed features, namely the eigenvalues of the hessian matrix and the structure tensor. These features have the advantage of being rotationally invariant and being good descriptors of most biologically relevant structures such as membranes, tubes, and vesicles.

Feature images are computed by computing hessian matrix and structure tensor eigenvalues, as well as down-sampling by average binning. Figure FigDeepConvExample shows an example of how feature images are comuted that, in this case, help to segment a line of dots. The names of the respective features indicate whether either the hessian matrix (He) or the structure tensor (St) was computed and which eigenvalue was computed, the largest (L), the middle (M), or the smallest (S). In addition the name contains information about the current binning relativ to the original image. For example, 9x9x3_StM_3x3x1_StS_Orig means that first the smallest eigenvalue of the structure tensor (StS)  was computed followed by a 3x3x1 average binning, followed by computing the middle eigenvalue of the structure tensor (StM), followed by a 3x3x3 binnning yielding a 9x9x3 binned image with respect to the original image.

### Dealing with anisotropic data

The algorithm does not always bin isotropically in order to account for a potential anisotropy of the input data. For example, if the resolution of the input data is 200 nm in x/y and 600 nm in z, and the cose down-sampling factor is 3, the first binning would be 3x3x1, yielding isotropic data with a (600 nm)^3 voxel size. The next binnings would be isotropic. This has the advantage that the data becomes very quickly isotropic, such that features computed in lower resolution levels are really 3-D. In addition the implementation of the Hessian matrix and Structure tensor also properly handles anisotropic image calibration (Ref: ImageScience).

## Random forest settings

A random forest has the following settings:
- N.. number of trees
- F.. number of random features per node
- ...

We chose F to be on tenth of the number of input features. Our intuition was that 1/10 is high enough to fetch the best features at each node with a decent probability, leading to a good classification strength of each tree, and low enough to have reasonably uncorrelated trees (the probability to have two sucessive nodes in two different trees using the same feature combination only is (1/10)^2 = 1/100). Both high strength and low correlation or important for a random forest classifier to work well [Ref Breiman].  

## User adjustable parameters

### Down-sample factor

The down-sample factor (DSF) determines how much the images are down-sampled from one resolution level (L) to the next. The optimal choice depends on the spatial structure of the input data. In general, a smaller value captures more details while a lager value faster approaches larger spatial scales. For example choosing a downsample factor of 2 yields binnings 2^(L) (1,2,4,8,...) while 3 yields binnings of 3^(L) (1,3,9,27,...); for the data shown in this article we used a down-sample factor of 3. 

### Maximal feature depth
  
In our current architecture we have 6 features (4 for 2-D images), namely 3 (2) eigenvalues of the hessian matrix and 3 (2) eigenvalues of the structure tensor. In addition, we allow also simply keep the downsampled version of each image without an additional feature computed. Thus, at each resolution level there are 7 (5) images that could be derived from all images in the preceeding resolution layer, yielding for the 3-D case 7^(L+1) features (L0:7, L1:49, L2:343, L3:2401, L4:16807, ...) at downsampling level L. Obviously this is a fast growing number, which for instance slows down the RF training that has to test all those features for their usefulness given the current classification task. To keep the numbers of features at bay we thus introduced a maximal feature depth (MFD) which determines how many levels of "features of features" are allowed. For example, given a downsample factor of 3 a feature image at level 2 could be 9x9x9_HeL_3x3x3_StS_1x1x1_Orig. Given a MCD of 2, this feature image could not be subjected to any further filters, but could only be further downsampled. However the image 9x9x9_3x3x3_StS_1x1x1_Orig would be subjected to additional filters, because it was so far only subjected to one filter, namely the smallest eigenvalue of the structure tensor (StS) at resolution level 0.

### Future ideas

#### Evaluate trees based on confidence

Sucessivley evaluate the next tree and stop if the confidence for a certain class reached a certain threshold. 

##
## Results
##

### C.elegans

- We have manual ground truth => we can give accuracies

### ISBI data set

- I guess there is a ground truth?!


##  
## Discussion
##


In neural network implementations of deep convolutions all convolutions are learned during the training. This has the advantage that the NN has the chance to learn the optimal convolutional filters for the given segmentation task. However this comes at a cost of many parameters and long training times. For instance, the 3-D U-Net has about 11 million parameters and took 3 days to train [Ref Ronneberger 3-D UNet]. In addition, deep convolutions encoded by NNs are intrinsically not rotationally invariant such that all rotations need to be explicitely learned and encoded by the NN. Especially in 3D this means that a lot of angles to be learned. As most biological data is rotationally invariant we feel that this is a disadvantage of NNs as compared with our approach where we only use rotationally invariant features. 

###

 
