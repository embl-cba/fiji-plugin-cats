

	public Instances getInstancesFromLabelImage(
			String instancesName,
			FinalInterval interval,
			int labelImageNumInstancesPerClass )
	{
		logger.info("Creating instances from label image... ");

		final long start = System.currentTimeMillis();

		Instances instances =
				InstancesCreator.
				createInstancesFromLabelImageRegion2(
						this,
						inputImage,
						labelImage,
						resultImage,
						instancesName,
						interval,
						labelImageNumInstancesPerClass,
						Prefs.getThreads(),
						logger);

		final long end = System.currentTimeMillis();
		logger.info("...created training data from label image in " + (end - start) + " ms");

		return ( instances );
	}


	/**
	 * Apply current classifier to set of instances
	 * @param data set of instances
	 * @param w image width
	 * @param h image height
	 * @param numThreads The number of threads to use. Set to zero for
	 * auto-detection.
	 * @return result image
	 */
	public ImagePlus applyClassifier(final Instances data, int w, int h, int numThreads, boolean probabilityMaps)
	{
		if (numThreads == 0)
			numThreads = Prefs.getThreads();

		final int numClasses   = data.numClasses();
		final int numInstances = data.numInstances();
		final int numChannels  = (probabilityMaps ? numClasses : 1);
		final int numSlices    = (numChannels*numInstances)/(w*h);

		IJ.showStatus("Classifying image...");

		final long start = System.currentTimeMillis();

		ExecutorService exe = Executors.newFixedThreadPool(numThreads);
		final double[][][] results = new double[numThreads][][];
		final Instances[] partialData = new Instances[numThreads];
		final int partialSize = numInstances / numThreads;
		Future<double[][]>[] fu = new Future[numThreads];

		final AtomicInteger counter = new AtomicInteger();

		for(int i = 0; i < numThreads; i++)
		{
			if (Thread.currentThread().isInterrupted())
			{
				exe.shutdown();
				return null;
			}
			if(i == numThreads - 1)
				partialData[i] = new Instances(data, i*partialSize, numInstances - i*partialSize);
			else
				partialData[i] = new Instances(data, i*partialSize, partialSize);

			AbstractClassifier classifierCopy = null;
			try {
				// The Weka random forest classifiers do not need to be duplicated on each thread
				// (that saves much memory)
				if( classifier instanceof FastRandomForest || classifier instanceof RandomForest )
					classifierCopy = classifier;
				else
					classifierCopy = (AbstractClassifier) (AbstractClassifier.makeCopy( classifier ));

			} catch (Exception e) {
				IJ.log("Error: classifier could not be copied to classify in a multi-thread way.");
				e.printStackTrace();
			}
			fu[i] = exe.submit(classifyInstances(partialData[i], classifierCopy, counter, probabilityMaps));
		}

		ScheduledExecutorService monitor = Executors.newScheduledThreadPool(1);
		ScheduledFuture task = monitor.scheduleWithFixedDelay(new Runnable() {
			@Override
			public void run() {
				IJ.showProgress(counter.get(), numInstances);
			}
		}, 0, 1, TimeUnit.SECONDS);

		// Join threads
		for(int i = 0; i < numThreads; i++)
		{
			try {
				results[i] = fu[i].get();
			} catch (InterruptedException e) {
				//e.printStackTrace();
				return null;
			} catch (ExecutionException e) {
				e.printStackTrace();
				return null;
			} finally {
				exe.shutdown();
				task.cancel(true);
				monitor.shutdownNow();
				IJ.showProgress(1);
			}
		}

		exe.shutdown();

		// Create final array
		double[][] classificationResult;
		classificationResult = new double[numChannels][numInstances];

		for(int i = 0; i < numThreads; i++)
			for (int c = 0; c < numChannels; c++)
				System.arraycopy(results[i][c], 0, classificationResult[c], i*partialSize, results[i][c].length);

		IJ.showProgress(1.0);
		final long end = System.currentTimeMillis();
		IJ.log("Classifying whole image data took: " + (end-start) + "ms");

		double[]         classifiedSlice = new double[w*h];
		final ImageStack classStack      = new ImageStack(w, h);

		for (int i = 0; i < numSlices/numChannels; i++)
		{
			for (int c = 0; c < numChannels; c++)
			{
				System.arraycopy(classificationResult[c], i*(w*h), classifiedSlice, 0, w*h);
				ImageProcessor classifiedSliceProcessor = new FloatProcessor(w, h, classifiedSlice);
				classStack.addSlice(probabilityMaps ? getClassLabels()[c] : "", classifiedSliceProcessor);
			}
		}
		ImagePlus classImg = new ImagePlus(probabilityMaps ? "Probability maps" : "Classification result", classStack);

		return classImg;
	}



	/**
	 * Apply current classifier to a given image with precomputed features.
	 *
	 * @param imp image (2D single image or stack)
	 * @param filters stack of filters to apply to the original image in order to create the features
	 * @param numThreads The number of threads to use. Set to zero for auto-detection.
	 * @param probabilityMaps create probability maps for each class instead of a classification
	 * @return result image
	 */
	public ImagePlus applyClassifier(
			final ImagePlus imp,
			final ImagePlus filters,
			int numThreads,
			final boolean probabilityMaps)
	{
		IJ.showMessage("applyClassifier: not implemented.");
		return (null);
		//return applyClassifier(imp, new FeatureImages(imp), numThreads, probabilityMaps);
	}

	/**
	 * Apply current classifier to a given image with precomputed features.
	 *
	 * @param imp image (2D single image or stack)
	 * @param featureImages precomputed feature stack array
	 * @param numThreads The number of threads to use. Set to zero for auto-detection.
	 * @param probabilityMaps create probability maps for each class instead of a classification
	 * @return result image
	 */
	public ImagePlus applyClassifier(
			final ImagePlus imp,
			FeatureImages featureImages,
			int numThreads,
			final boolean probabilityMaps)
	{

		IJ.showMessage( "not implemented" );


		if ( false )
		{

			if (numThreads == 0)
				numThreads = Prefs.getThreads();

			final int numSliceThreads = Math.min(imp.getStackSize(), numThreads);
			final int numClasses = numOfClasses;
			final int numChannels = (probabilityMaps ? numClasses : 1);

			IJ.log("Processing slices of " + imp.getTitle() + " in " + numSliceThreads + " thread(s)...");

			// Set proper class names (skip empty list ones)
			ArrayList<String> classNames = new ArrayList<String>();
			if (null == loadedClassNames)
			{
				for (int i = 0; i < numOfClasses; i++)
					for (int j = 0; j < trainingImage.getImageStackSize(); j++)
						if ( getNumExamples( i ) > 0 )
						{
							classNames.add(getClassLabels()[i]);
							break;
						}
			}
			else
				classNames = loadedClassNames;

			final ImagePlus[] classifiedSlices = new ImagePlus[imp.getStackSize()];

			class ApplyClassifierThread extends Thread {

				private final int startSlice;
				private final int numSlices;
				private final int numFurtherThreads;
				private final ArrayList<String> classNames;
				private final FeatureImages fsa;

				public ApplyClassifierThread(
						int startSlice,
						int numSlices,
						int numFurtherThreads,
						ArrayList<String> classNames,
						FeatureImages fsa)
				{

					this.startSlice = startSlice;
					this.numSlices = numSlices;
					this.numFurtherThreads = numFurtherThreads;
					this.classNames = classNames;
					this.fsa = fsa;
				}

				@Override
				public void run()
				{
				/*
				for (int i = startSlice; i < startSlice + numSlices; i++)
				{
					final ImagePlus slice = new ImagePlus(imp.getImageStack().getSliceLabel(i), imp.getImageStack().getProcessor(i));

					final Instances sliceData = featureImages.createInstances(classNames);
					sliceData.setClassIndex(sliceData.numAttributes() - 1);

					IJ.log("Classifying slice " + i + " in " + numFurtherThreads + " thread(s)...");
					final ImagePlus classImage = applyClassifier(sliceData, slice.getWidth(), slice.getHeight(), numFurtherThreads, probabilityMaps);

					if( null == classImage )
					{
						IJ.log("Error while applying classifier!");
						return;
					}

					classImage.setTitle("classified_" + slice.getTitle());
					if(probabilityMaps)
						classImage.setProcessor(classImage.getProcessor().duplicate());
					else
						classImage.setProcessor(
								classImage.getProcessor().convertToByte(
										false ).duplicate());
					classifiedSlices[i-1] = classImage;
				}
				*/
				}
			}

			final int numFurtherThreads = (int) Math.ceil((double) (numThreads - numSliceThreads) / numSliceThreads) + 1;

			final ApplyClassifierThread[] threads = new ApplyClassifierThread[numSliceThreads];

			// calculate optimum number of slices per thread
			int[] numSlicesPerThread = new int[numSliceThreads];
			for (int i = 0; i < imp.getImageStackSize(); i++)
			{
				numSlicesPerThread[i % numSliceThreads]++;
			}

			int aux = 0;
			for (int i = 0; i < numSliceThreads; i++)
			{

				int startSlice = aux + 1;

				aux += numSlicesPerThread[i];

				IJ.log("Starting thread " + i + " processing " + numSlicesPerThread[i] + " slices, starting with " + startSlice);
				threads[i] = new ApplyClassifierThread(startSlice, numSlicesPerThread[i], numFurtherThreads, classNames, featureImages);

				threads[i].start();
			}

			// create classified image
			final ImageStack classified = new ImageStack(imp.getWidth(), imp.getHeight());

			// join threads
			for (Thread thread : threads)
				try
				{
					thread.join();
				} catch (InterruptedException e)
				{
					e.printStackTrace();
				}

			// assemble classified image
			for (int i = 0; i < imp.getStackSize(); i++)
				for (int c = 0; c < numChannels; c++)
					classified.addSlice("", classifiedSlices[i].getStack().getProcessor(c + 1));

			ImagePlus result = new ImagePlus("Classification result", classified);

			if (probabilityMaps)
			{
				result.setDimensions(numOfClasses, imp.getNSlices(), imp.getNFrames());
				if (imp.getNSlices() * imp.getNFrames() > 1)
					result.setOpenAsHyperStack(true);
			}

			result.setCalibration(trainingImage.getCalibration());
			return result;
		}

		return null;

	}

	/**
	 * Apply current classifier to current image. Classification is performed
	 * in a multi-threaded way, using as many threads as defined by the user
	 * in the ImageJ options. Note: all image features are first calculated in
	 * parallel and then the classifier is applied.
	 *
	 * @param classify flag to get labels or probability maps (false = labels)
	 */
	/*
	public void applyClassifier( boolean probabilityMaps, Region5D region5D, ImagePlus classifiedImage )
	{
		if( Thread.currentThread().isInterrupted() )
		{
			IJ.log("Classification was interrupted by the user.");
			return;
		}
		applyClassifier(0, probabilityMaps, region5D, classifiedImage);
	}*/

	/**
	 * Apply current classifier to current image. Note: all image features are
	 * first calculated in parallel and then the classifier is applied.
	 *
	 * @param numThreads The number of threads to use. Set to zero for
	 * auto-detection (defined by the user in the ImageJ options).
	 * @param probabilityMaps flag to get labels or probability maps (false = labels)
	 */
	/*
	public void applyClassifier( int numThreads, boolean probabilityMaps )
	{
		if( Thread.currentThread().isInterrupted() )
		{
			IJ.log("Training was interrupted by the user.");
			return;
		}

		if (numThreads == 0)
			numThreads = Prefs.getThreads();

		IJ.log("Classifying whole image using " + numThreads + " thread(s)...");
		try{
			classifiedImage = applyClassifier(trainingImage, numThreads, probabilityMaps );
		}
		catch(Exception ex)
		{
			IJ.log("Error while classifying whole image! ");
			ex.printStackTrace();
		}

		IJ.log("Finished segmentation of whole image.\n");
	}*/


		/**
    	 * Apply current classifier to a given image. It divides the
    	 * whole slices of the input image into the selected number of threads.
    	 * Each thread will sequentially process a whole  number of slices (first
    	 * feature calculation then classification).
    	 *
    	 * @param imp image (2D single image or stack)
    	 * @param numThreads The number of threads to use. Set to zero for
    	 * auto-detection (set by the user on the ImageJ preferences)
    	 * @param probabilityMaps create probability maps for each class instead of
    	 * a classification
    	 * @return result image
    	 */
    	/*
    	public ImagePlus applyClassifier(
    			final ImagePlus imp,
    			Region5D region5D, // where to do the classification
    			//ImagePlus classImg, // classification results
    			boolean updateFeatureImages,
    			int numThreads,
    			final boolean probabilityMaps)
    	{
    		if (numThreads == 0)
    			numThreads = Prefs.getThreads();

    		ImagePlus inputImage = null;

    		if ( region5D != null )
    		{
    			if (imp.getImageStack() instanceof VirtualStackOfStacks)
    			{
    				VirtualStackOfStacks vss = (VirtualStackOfStacks) imp.getStack();
    				inputImage = vss.getDataCube(region5D, 0, numThreads);
    			}
    			else
    			{
    				IJ.showMessage("no VSS");
    				return null;
    			}
    		}
    		else
    		{
    			inputImage = imp;
    		}

    		if ( updateFeatureImages )
    		{
    			featureImages.setOriginalImage(inputImage);
    			featureImages.updateFeaturesMT();
    		}

    		final ImagePlus[] results = new ImagePlus[imp.getNFrames()];

    		IJ.log("Classifying frame "+frame);

    		results[region5D.t] =
    				applyClassifier(featureImages, region5D.t + 1, numThreads, probabilityMaps);

    		if (probabilityMaps)
    		{
    			results[region5D.t].setDimensions(
    					numOfClasses, imp.getNSlices(), imp.getNFrames());
    			if( imp.getNSlices() * imp.getNFrames() > 1)
    				results[region5D.t].setOpenAsHyperStack(true);
    		}

    		results[region5D.t].setCalibration(trainingImage.getCalibration());


    		// TODO: make hyperstack from multiple frames

    		// force garbage collection
    		//featureImages = null;
    		System.gc();
    		return results[0]; // TODO: implement for multiple frames

    		/*
    		final int numSliceThreads = Math.min(imp.getStackSize(), numThreads);
    		final int numClasses      = numOfClasses;
    		final int numChannels     = (probabilityMaps ? numClasses : 1);

    		IJ.log("Processing slices of " + imp.getTitle() + " in " + numSliceThreads + " thread(s)...");

    		// Set proper class names (skip empty list ones)
    		ArrayList<String> classNames = new ArrayList<String>();
    		if( null == loadedClassNames )
    		{
    			for(int i = 0; i < numOfClasses; i++)
    				for(int j=0; j<trainingImage.getImageStackSize(); j++)
    					if(!examples[j].get(i).isEmpty())
    					{
    						classNames.add(getClassLabels()[i]);
    						break;
    					}
    		}
    		else
    			classNames = loadedClassNames;

    		final ImagePlus[] classifiedSlices = new ImagePlus[imp.getStackSize()];

    		class ApplyClassifierThread extends Thread
    		{

    			private final int startSlice;
    			private final int numSlices;
    			private final int numFurtherThreads;
    			private final ArrayList<String> classNames;

    			public ApplyClassifierThread(
    					int startSlice,
    					int numSlices,
    					int numFurtherThreads,
    					ArrayList<String> classNames)
    			{

    				this.startSlice         = startSlice;
    				this.numSlices          = numSlices;
    				this.numFurtherThreads  = numFurtherThreads;
    				this.classNames         = classNames;
    			}

    			@Override
    			public void run()
    			{

    				for (int i = startSlice; i < startSlice + numSlices; i++)
    				{
    					final ImagePlus slice = new ImagePlus(imp.getImageStack().getSliceLabel(i), imp.getImageStack().getProcessor(i));
                        // Create feature stack for slice
                        IJ.showStatus("Creating features...");
                        IJ.log("Creating features for slice " + i +  "...");
                        FeatureStack sliceFeatures = new FeatureStack(slice);
                        // Use the same features as the current classifier
                        sliceFeatures.setEnabledFeatures( enabledFeatures );
                        sliceFeatures.setMaximumSigma(maximumSigma);
                        sliceFeatures.setMinimumSigma(minimumSigma);
                        sliceFeatures.setMembranePatchSize(membranePatchSize);
                        sliceFeatures.setMembraneSize(membraneThickness);
                        sliceFeatures.updateFeaturesST();
                        filterFeatureStackByList(featureNames, sliceFeatures);
                        Instances sliceData = sliceFeatures.createInstances(classNames);
                        sliceData.setClassIndex(sliceData.numAttributes() - 1);

    					IJ.log("Classifying slice " + i + " in " + numFurtherThreads + " thread(s)...");
    					final ImagePlus classImage = applyClassifier(sliceData, slice.getWidth(), slice.getHeight(), numFurtherThreads, probabilityMaps);

    					if( null == classImage )
    					{
    						IJ.log("Error while applying classifier!");
    						return;
    					}
    					classImage.setCalibration( imp.getCalibration() );
    					classImage.setTitle("classified_" + slice.getTitle());

    					classifiedSlices[i-1] = classImage;
    					// force garbage collection
    					sliceFeatures = null;
    					sliceData = null;
    					System.gc();
    				}
    			}
    		}

    		final int numFurtherThreads = (int)Math.ceil((double)(numThreads - numSliceThreads)/numSliceThreads) + 1;
    		final ApplyClassifierThread[] threads = new ApplyClassifierThread[numSliceThreads];

    		// calculate optimum number of slices per thread
    		int[] numSlicesPerThread = new int [ numSliceThreads ];
    		for(int i=0; i<imp.getImageStackSize(); i++)
    		{
    			numSlicesPerThread[ i % numSliceThreads ] ++;
    		}

    		int aux = 0;
    		for (int i = 0; i < numSliceThreads; i++)
    		{

    			int startSlice = aux + 1;

    			aux += numSlicesPerThread[ i ];

    			IJ.log("Starting thread " + i + " processing " + numSlicesPerThread[ i ] + " slices, starting with " + startSlice);
    			threads[i] = new ApplyClassifierThread(startSlice, numSlicesPerThread[ i ], numFurtherThreads, classNames );

    			threads[i].start();
    		}

    		// create classified image
    		final ImageStack classified = new ImageStack(imp.getWidth(), imp.getHeight());

    		// join threads
    		for(Thread thread : threads)
    			try {
    				thread.join();
    			} catch (InterruptedException e) {
    				e.printStackTrace();
    			}

    		// assemble classified image
    		for (int i = 0; i < imp.getStackSize(); i++)
    			for (int c = 0; c < numChannels; c++)
    				classified.addSlice("", classifiedSlices[i].getStack().getProcessor(c+1));

    		ImagePlus result = new ImagePlus("Classification result", classified);

    		if (probabilityMaps)
    		{
    			result.setDimensions(numOfClasses, imp.getNSlices(), imp.getNFrames());
    			if (imp.getNSlices()*imp.getNFrames() > 1)
    				result.setOpenAsHyperStack(true);
    		}
    		result.setCalibration(imp.getCalibration());

    		return result;
    	}*/

	/**
	 * Apply current classifier to a given image. If the input image is a
	 * stack, the classification task will be carried out by slice in
	 * parallel by each available thread (number of threads read on
	 * Prefs.getThreads()). Each thread will sequentially process a whole
	 * number of slices (first feature calculation then classification).
	 *
	 * @param imp image (2D single image or stack)
	 * @return result image (classification)
	 */
	public ImagePlus applyClassifierFullImage(final ImagePlus imp)
	{
		IJ.log("not implemented");
		return null;
		//return applyClassifier(imp, 0, false);
	}


	/**
	 * Apply current classifier to a given image in a complete concurrent way.
	 * This method is experimental, it divides the image(s) in pieces and
	 * can cause artifacts using some filters.
	 *
	 * @param imp image (2D single image or stack)
	 * @param numThreads The number of threads to use. Set to zero for
	 * auto-detection.
	 * @param probabilityMaps create probability maps for each class instead of
	 * a classification
	 * @return result image
	 */
	public ImagePlus applyClassifierMT(
			final ImagePlus imp,
			int numThreads,
			final boolean probabilityMaps)
	{
		if (numThreads == 0)
			numThreads = Prefs.getThreads();


		final int numClasses = numOfClasses;
		final int numChannels = (probabilityMaps ? numClasses : 1);

		IJ.log("Classifying data from image " + imp.getTitle() + " using " + numThreads + " thread(s)...");

		// Set proper class names (skip empty list ones)
		ArrayList<String> classNames = new ArrayList<String>();
		if( null == loadedClassNames )
		{
			for(int i = 0; i < numOfClasses; i++)
				for(int j=0; j<trainingImage.getImageStackSize(); j++)
					if( getNumExamples( i ) > 0 )
					{
						classNames.add(getClassLabels()[i]);
						break;
					}
		}
		else
			classNames = loadedClassNames;

		// Create instances information (each instance needs a pointer to this)
		ArrayList<Attribute> attributes = new ArrayList<Attribute>();
		for (int i=1; i<= featureImages.getNumFeatures(); i++)
		{
			String attString = featureImages.getLabel(i);
			attributes.add(new Attribute(attString));
		}

		attributes.add(new Attribute("class", classNames));
		Instances dataInfo = new Instances("segment", attributes, 1);
		dataInfo.setClassIndex(dataInfo.numAttributes()-1);

		final long start = System.currentTimeMillis();

		// Initialize executor service
		if(exe.isShutdown())
			exe = Executors.newFixedThreadPool(numThreads);


		// counter to display the progress
		final AtomicInteger counter = new AtomicInteger();

		// slice dimensions
		final int height = imp.getHeight();
		final int width = imp.getWidth();
		final int pad = (int) maximumSigma;

		// Calculate number of rows per thread
		// (with this division we may miss one row,
		// but it will be added to the last thread)
		int numOfRows = height * imp.getImageStackSize() / numThreads;

		// set each slice in a thread
		Future<ArrayList <ImagePlus> >[] fu = new Future [ numThreads ];

		ArrayList<int[]> imagePad = new ArrayList<int[]>();

		ArrayList <ImagePlus>[] list = new ArrayList [ numThreads ];

		// Divide work among available threads
		//IJ.log("Dividing image data among the " + numThreads + " available threads...");
		//final long time1 = System.currentTimeMillis();
		for(int i = 0; i < numThreads; i++)
		{
			list[ i ] = new ArrayList < ImagePlus > ();
			if (Thread.currentThread().isInterrupted())
				return null;

			// Calculate list of images to be classified on each thread
			int firstRow = i * numOfRows;
			int lastRow = i < (numThreads-1) ? (i+1) * numOfRows - 1 : height * imp.getImageStackSize()-1;

			//IJ.log("Thread " + i + ": first row = " + firstRow + ", last row = " + lastRow);


			int r = firstRow;
			int rowsToDo = lastRow - firstRow + 1;

			while( r < lastRow )
			{
				final int slice = r / height;
				final int begin = r - slice * height;

				final int end = (begin + rowsToDo) > height ? height-1 : begin + rowsToDo-1;

				// Create image
				ImageProcessor sliceImage = imp.getImageStack().getProcessor(slice+1);

				// We pad the images if necessary (for the filtering)
				final int paddedBegin = begin - pad;
				final int paddedEnd = end + pad;

				// Crop the area of the slice that will be process on this thread
				sliceImage.setRoi(new Rectangle(0, paddedBegin, width, paddedEnd-paddedBegin+1 ) );
				ImageProcessor im = sliceImage.crop();

				final ImagePlus ip = new ImagePlus( "slice-" + slice + "-" + begin, im);
				// add image to list
				list[ i ].add( ip );

				//IJ.log(" begin = " + begin + ", end = " + end + ", paddedBegin = " + paddedBegin + ", paddedEnd = " + paddedEnd + ", height = " + height + ", pad = " + pad);

				// We store the padding number to recover the area of interest later
				final int padTop = (paddedBegin >= 0) ? pad : pad + paddedBegin ;
				final int padBottom = (paddedEnd < height) ? pad : pad - (paddedEnd - height + 1);

				//IJ.log(" padTop = " + padTop + ", padBottom = " + padBottom );

				imagePad.add( new int[]{slice,  		/* slice number (starting at 0) */
										padTop, 		/* top padding */
										padBottom, 		/* bottom padding */
										end-begin+1} );	/* size (number of rows) */

				int rowsDone = end-begin+1;
				r += rowsDone;
				rowsToDo -= rowsDone;
			}



		}
		//final long time2 = System.currentTimeMillis();
		//IJ.log(" Done. Image division took " + (time2-time1)  + " ms.");


		// Create a copy of the classifier for each thread
		AbstractClassifier[] classifierCopy = new AbstractClassifier[ numThreads ];
		IJ.log("Creating classifier copy for each thread...");
		for(int i = 0; i < numThreads; i++)
		{

			try {
				// The Weka random forest classifiers do not need to be duplicated on each thread
				// (that saves much memory)
				if( classifier instanceof FastRandomForest || classifier instanceof RandomForest )
					classifierCopy[ i ] = classifier;
				else
					classifierCopy[ i ] = (AbstractClassifier) (AbstractClassifier.makeCopy( classifier ));

			} catch (Exception e) {
				IJ.log("Error: classifier could not be copied to classify in a multi-thread way.");
				e.printStackTrace();
				return null;
			}

		}
		//final long time3 = System.currentTimeMillis();
		//IJ.log(" Done. Classifiers duplication took " + (time3-time2)  + " ms.");

		// Submit the jobs
		for(int i = 0; i < numThreads; i++)
		{
			// classify slice
			fu[i] = exe.submit( classifyListOfImages( list[ i ] , dataInfo, classifierCopy[ i ], counter, probabilityMaps ));
		}

		final int numInstances = imp.getHeight() * imp.getWidth() * imp.getStackSize();

		ScheduledExecutorService monitor = Executors.newScheduledThreadPool(1);
		ScheduledFuture task = monitor.scheduleWithFixedDelay(new Runnable() {
			@Override
			public void run() {
				IJ.showProgress(counter.get(), numInstances);
			}
		}, 0, 1, TimeUnit.SECONDS);

		// array of images to store the classification results
		final ArrayList< ImagePlus> classifiedImages = new ArrayList < ImagePlus > ();

		// Join threads
		for(int i = 0; i < numThreads; i++)
		{
			try {
				ArrayList<ImagePlus> result = fu[i].get();
				for(ImagePlus ip : result)
				{
					classifiedImages.add( ip );
				}
			} catch (Exception e) {
				e.printStackTrace();
				return null;
			}
			catch ( OutOfMemoryError err ) {
				IJ.log( "ERROR: applyClassifierMT run out of memory. Please, "
						+ "use a smaller input image or fewer features." );
				err.printStackTrace();
				return null;
			} finally {
				task.cancel(true);
				monitor.shutdownNow();
				IJ.showProgress(1);
			}
		}

		// create classified image
		final ImageStack classified = new ImageStack(imp.getWidth(), imp.getHeight() );
		for(int i=0; i < imp.getStackSize(); i++)
		{
			if(numChannels > 1)
			{
				for (int c = 0; c < numChannels; c++)
					classified.addSlice("", new FloatProcessor(width, height));
			}
			else
				classified.addSlice("", new ByteProcessor(width, height));
		}

		// assemble classified image
		int n = 0;
		int raw = 0;
		for( final ImagePlus ip : classifiedImages )
		{
			raw = raw % height;

			//ip.show();

			int[] coord = imagePad.get( n );

			//final int sliceNum = coord[ 0 ] + 1;
			final int beginPad = coord[ 1 ];
			final int endPad = coord[ 2 ];
			final int size = coord[ 3 ];
			//IJ.log(" coord[0] = " + coord[0] + ", coord[1] = " + coord[1] + ", coord[2] = " + coord[2] + ", coord[3] = " + coord[3] );

			for (int c = 0; c < numChannels; c++)
			{
				// target image
				ImageProcessor target = classified.getProcessor(coord[ 0 ] * numChannels + c + 1);
				// source image
				ImageProcessor source = ip.getImageStack().getProcessor(c+1);
				//IJ.log(" set roi = 0, " + beginPad + ", " + width + ", " + (ip.getHeight() - endPad));
				source.setRoi( new Rectangle( 0, beginPad, width, ip.getHeight() - endPad));
				source = source.crop();
				// copy
				target.copyBits(source, 0, raw, Blitter.COPY);
			}
			raw += size;
			n++;
		}




		ImagePlus result = new ImagePlus("Classification result", classified);

		if (probabilityMaps)
		{
			result.setDimensions(numOfClasses, imp.getNSlices(), imp.getNFrames());
			if (imp.getNSlices()*imp.getNFrames() > 1)
				result.setOpenAsHyperStack(true);
		}

		final long end = System.currentTimeMillis();
		IJ.log("Whole image classification took " + (end-start) + " ms.");
		return result;
	}


	/**
	 * Classify a slice in a concurrent way
	 * @param slice image to classify
	 * @param dataInfo empty set of instances containing the data structure (attributes and classes)
	 * @param classifier classifier to use
	 * @param counter counter used to display the progress in the tool bar
	 * @param probabilityMaps flag to calculate probabilities or binary results
	 * @return classification result
	 */
	public Callable<ImagePlus> classifySlice(
			final ImagePlus slice,
			final Instances dataInfo,
			final AbstractClassifier classifier,
			final AtomicInteger counter,
			final boolean probabilityMaps)
	{
		if (Thread.currentThread().isInterrupted())
			return null;

		return new Callable<ImagePlus>(){
			@Override
			public ImagePlus call()
			{
				// Create feature stack for slice
				IJ.showStatus("Creating features...");
				IJ.log("Creating features of slice " + slice.getTitle() + "...");
				final FeatureStack sliceFeatures = new FeatureStack(slice);
				// Use the same features as the current classifier
				sliceFeatures.setEnabledFeatures(enabledFeatures);
				sliceFeatures.setMaximumSigma(maximumSigma);
				sliceFeatures.setMinimumSigma(minimumSigma);
				sliceFeatures.setMembranePatchSize(membranePatchSize);
				sliceFeatures.setMembraneSize(membraneThickness);
				if(!sliceFeatures.updateFeaturesST())
				{
					IJ.log("Classifier execution was interrupted.");
					return null;
				}

				final int width = slice.getWidth();
				final int height = slice.getHeight();
				final int numClasses = dataInfo.numClasses();

				ImageStack classificationResult = new ImageStack(width, height);

				final int numInstances = width * height;

				final double[][] probArray;

				if (probabilityMaps)
					probArray = new double[numClasses][numInstances];
				else
					probArray = new double[1][numInstances];

				IJ.log("Classifying slice " + slice.getTitle() + "...");

				// auxiliary array to be filled for each instance
				final int extra = sliceFeatures.useNeighborhood() ? 8 : 0;
				final double[] values =
						new double[ sliceFeatures.getSize() + 1 + extra ];
				// create empty reusable instance
				final ReusableDenseInstance ins =
						new ReusableDenseInstance( 1.0, values );
				ins.setDataset( dataInfo );

				for (int x=0; x<width; x++)
					for(int y=0; y<height; y++)
					{
						try{

							if (0 == (x+y*width) % 4000)
							{
								if (Thread.currentThread().isInterrupted())
									return null;
								counter.addAndGet(4000);
							}

							sliceFeatures.setInstance( x, y, 0, ins, values );

							if (probabilityMaps)
							{
								double[] prob = classifier.distributionForInstance( ins );
								for(int k = 0 ; k < numClasses; k++)
								{
									probArray[k][x+y*width] = prob[ k ];
								}
							}
							else
							{
								probArray[0][ x+y*width ] = classifier.classifyInstance( ins );
							}

						}catch(Exception e){

							IJ.showMessage("Could not apply Classifier!");
							e.printStackTrace();
							return null;
						}
					}

				if( probabilityMaps )
				{
					for(int k = 0 ; k < numClasses; k++)
						classificationResult.addSlice("class-" + (k+1), new FloatProcessor(width, height, probArray[k]) );
				}
				else
					classificationResult.addSlice("result", new FloatProcessor(width, height, probArray[0]) );

				return new ImagePlus("classified-slice", classificationResult);
			}
		};
	}



	/**
	 * Create the whole image data (instances) from the current image and feature stack.
	 *
	 * @return feature vectors (Weka instances) of the entire image
	 */
	public Instances updateWholeImageData()
	{
		IJ.showMessage("not implemented");

		if ( false )
		{
			Instances wholeImageData = null;

			IJ.showStatus("Reading whole image data...");
			IJ.log("Reading whole image data...");

			long start = System.currentTimeMillis();
			ArrayList<String> classNames = null;

			if (null != loadedClassNames)
				classNames = loadedClassNames;
			else
			{
				classNames = new ArrayList<String>();

				for (int j = 0; j < trainingImage.getImageStackSize(); j++)
					for (int i = 0; i < numOfClasses; i++)
						if ( getNumExamples( i ) > 0  && !classNames.contains(getClassLabels()[i]))
						{
							classNames.add(getClassLabels()[i]);
						}
			}

			final int numProcessors = Prefs.getThreads();
			final ExecutorService exe = Executors.newFixedThreadPool(numProcessors);

			final ArrayList<Future<Instances>> futures = new ArrayList<Future<Instances>>();

			try
			{


			for(int z = 1; z<=trainingImage.getImageStackSize(); z++)
			{
				IJ.log("Creating feature vectors for slice number " + z + "...");
				//futures.add( exe.submit( createInstances(classNames, featureImages.get(z-1))) );
			}

			Instances[] data = new Instances[ futures.size() ];

			for(int z = 1; z<=trainingImage.getImageStackSize(); z++)
			{
				data[z-1] = futures.get(z-1).get();
				data[z-1].setClassIndex(data[z-1].numAttributes() - 1);
			}

			for(int n=0; n<data.length; n++)
			{
				//IJ.log("Test dataset updated ("+ data[n].numInstances() + " instances, " + data[n].numAttributes() + " attributes).");

				if(null == wholeImageData)
					wholeImageData = data[n];
				else
					mergeDataInPlace(wholeImageData, data[n]);
			}

			IJ.log("Total dataset: "+ wholeImageData.numInstances() +
					" instances, " + wholeImageData.numAttributes() + " attributes.");
			long end = System.currentTimeMillis();
			IJ.log("Creating whole image data took: " + (end-start) + "ms");


			}
			catch (InterruptedException e)
			{
				IJ.log("The data update was interrupted by the user.");
				IJ.showStatus("The data update was interrupted by the user.");
				IJ.showProgress(1.0);
				exe.shutdownNow();
				return null;
			} catch (Exception ex)
			{
				IJ.log("Error when updating data for the whole image test set.");
				ex.printStackTrace();
				exe.shutdownNow();
				return null;
			} finally
			{
				exe.shutdown();
			}

			return wholeImageData;
		}

		return null;

	}

	private int addThinFreeLineSamples(final Instances trainingData, Example example)
	{
		int numInstances = 0;
		int[] x = example.roi.getPolygon().xpoints;
		int[] y = example.roi.getPolygon().ypoints;
		final int n = example.roi.getPolygon().npoints;

		for (int i = 0; i < n; i++)
		{
			double[] values = new double[featureImages.getNumFeatures()+1];

			for (int iFeature = 0 ; iFeature < featureImages.getNumFeatures(); iFeature++)
				values[iFeature] = featureImages.getFeatureValue(
						x[i], y[i], example.z, example.t, iFeature);

			values[featureImages.getNumFeatures()] = example.classNum;
			trainingData.add(new DenseInstance(1.0, values));
			// increase number of instances for this class
			numInstances ++;
		}
		return numInstances;
	}

	/**
	 * Add training samples from a ShapeRoi
	 *
	 * @param trainingData set of instances to add to
	 * @param classIndex class index value
	 * @param sliceNum number of 2d slice being processed
	 * @param r shape roi
	 * @return number of instances added
	 */
	private int addShapeRoiInstances(
			final Instances trainingData,
			Example example)
	{
		int numInstances = 0;
		final ShapeRoi shapeRoi = new ShapeRoi(example.roi);
		final Rectangle rect = shapeRoi.getBounds();

		int lastX = rect.x + rect.width ;
		if( lastX >= trainingImage.getWidth() )
			lastX = trainingImage.getWidth() - 1;
		int lastY = rect.y + rect.height;
		if( lastY >= trainingImage.getHeight() )
			lastY = trainingImage.getHeight() - 1;
		int firstX = Math.max( rect.x, 0 );
		int firstY = Math.max( rect.y, 0 );

		//DenseInstance ins = new DenseInstance( featureImages.getNumFeatures() + 1 );
		//ins.setDataset( trainingData );

		for(int x = firstX; x < lastX; x++)
			for(int y = firstY; y < lastY; y++)
				if(shapeRoi.contains(x, y))
				{
					//fs.createInstanceInPlace( x, y, classIndex, ins );
					trainingData.add( featureImages.createInstance(
							x, y, example.z, example.t, example.classNum ) );

					// increase number of instances for this class
					numInstances ++;
				}
		return numInstances;
	}

	/**
	 * Add training samples from a rectangular roi
	 *
	 * @param trainingData set of instances to add to
	 * @param classIndex class index value
	 * @param sliceNum number of 2d slice being processed
	 * @param r shape roi
	 * @return number of instances added
	 */
	private int addRectangleRoiInstances(
			final Instances trainingData,
			Example example)
	{
		int numInstances = 0;

		final Rectangle rect = example.roi.getBounds();

		final int x0 = rect.x;
		final int y0 = rect.y;

		final int lastX = x0 + rect.width;
		final int lastY = y0 + rect.height;


		for( int x = x0; x < lastX; x++ )
			for( int y = y0; y < lastY; y++ )
			{
				trainingData.add( featureImages.createInstance(
						x, y, example.z, example.t, example.classNum ) );

				// increase number of instances for this class
				numInstances ++;
			}
		return numInstances;
	}

	/**
	 * Add training samples from a Line roi
	 *
	 * @param trainingData set of instances to add to
	 * @param colorFeatures color instances flag
	 * @param classIndex class index value
	 * @param sliceNum number of 2d slice being processed
	 * @param r Line roi
	 * @return number of instances added
	 */
	private int addLineInstances(
			final Instances trainingData,
			final boolean colorFeatures,
			Example example)
	{
		Roi r = example.roi;
		int numInstances = 0;
		double dx = ((Line)r).x2d - ((Line)r).x1d;
		double dy = ((Line)r).y2d - ((Line)r).y1d;
		int n = (int) Math.round( Math.sqrt( dx*dx + dy*dy ) );
		double xinc = dx/n;
		double yinc = dy/n;

		double x = ((Line)r).x1d;
		double y = ((Line)r).y1d;

		for (int i=0; i<n; i++)
		{
			double[] values = new double[featureImages.getNumFeatures()+1];

			for (int iFeature=0; iFeature < featureImages.getNumFeatures(); iFeature++)
				values[iFeature] = featureImages.getFeatureValue(
						(int)x, (int)y, example.z, example.t, iFeature );

			values[featureImages.getNumFeatures()] = example.classNum;
			trainingData.add(new DenseInstance(1.0, values));
			// increase number of instances for this class
			numInstances ++;

			x += xinc;
			y += yinc;
		}
		return numInstances;
	}

	/**
	 * Add training samples from a FreeLine roi with thickness larger than 1 pixel
	 *
	 * @param trainingData set of instances to add to
	 * @param colorFeatures color instances flag
	 * @param classIndex class index value
	 * @param sliceNum number of 2d slice being processed
	 * @param r FreeLine roi
	 * @return number of instances added
	 */
	private int addThickFreeLineInstances(final Instances trainingData,
			final boolean colorFeatures, Example example)
	{
		final int width = Math.round(example.roi.getStrokeWidth());
		FloatPolygon p = example.roi.getFloatPolygon();
		int n = p.npoints;

		int numInstances = 0;

		double x1, y1;
		double x2=p.xpoints[0]-(p.xpoints[1]-p.xpoints[0]);
		double y2=p.ypoints[0]-(p.ypoints[1]-p.ypoints[0]);
		for (int i=0; i<n; i++)
		{
			x1 = x2;
			y1 = y2;
			x2 = p.xpoints[i];
			y2 = p.ypoints[i];

			double dx = x2-x1;
			double dy = y1-y2;
			double length = (float)Math.sqrt(dx*dx+dy*dy);
			dx /= length;
			dy /= length;
			double x = x2-dy*width/2.0;
			double y = y2-dx*width/2.0;

			int n2 = width;
			do {
				if(x >= 0 && x < featureImages.getWidth()
						&& y >= 0 && y < featureImages.getHeight())
				{
					double[] values = new double[featureImages.getNumFeatures()+1];

					for (int iFeature = 0; iFeature < featureImages.getNumFeatures(); iFeature++)
						values[iFeature] = featureImages.getFeatureValue( (int)x, (int)y, example.z, example.t, iFeature );

					values[featureImages.getNumFeatures()] = example.classNum;
					trainingData.add(new DenseInstance(1.0, values));
					// increase number of instances for this class
					numInstances ++;
				}
				x += dy;
				y += dx;
			} while (--n2>0);
		}
		return numInstances;
	}


		/**
    	 * Save current feature stack(s)
    	 *
    	 * @param dir directory to save the stack(s)
    	 * @param fileWithExt file name with extension for the file(s)
    	 */
    	public static void saveFeatureStack(String dir, String fileWithExt)
    	{
    		final ImageWindow iw = WindowManager.getCurrentImage().getWindow();
    		if( iw instanceof CustomWindow )
    		{
    			final CustomWindow win = (CustomWindow) iw;
    			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();
    			final FeatureImages featureImages = wekaSegmentation.getFeatureImages();
    			if(featureImages.isEmpty())
    			{
    				featureImages.updateFeaturesMT();
    			}

    			if(null == dir || null == fileWithExt)
    				return;

    			for(int i=0; i< featureImages.getNumFeatures(); i++)
    			{
    				final String fileName = dir + fileWithExt.substring(0, fileWithExt.length()-4)
    				+ String.format("%04d", (i+1)) + ".tif";

    				if( !featureImages.saveStackAsTiff(fileName))
    				{
    					IJ.error("Error", "Feature stack could not be saved");
    					return;
    				}


    				IJ.log("Saved feature stack for slice " + (i+1) + " as " + fileName);
    			}
    		}
    	}


	/**
	 * Add instances to a specific class from a label (binary) image.
	 * Only white (non black) pixels will be added to the corresponding class.
	 *
	 * @param labelImage binary image
	 * @param featureImages corresponding feature stack
	 * @param className name of the class which receives the instances
	 * @return false if error
	 */
	public boolean addBinaryData(
			ImagePlus labelImage,
			FeatureImages featureImages,
			String className)
	{

		IJ.showMessage("NOT IMPLEMENTED: addBinaryData");

		// Update features if necessary
		if(featureImages.getNumFeatures() < 2)
		{
			IJ.log("Creating feature stack...");
			featureImages.updateFeaturesMT(showFeatureImages);
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}

		// Detect class index
		int classIndex = 0;
		for(classIndex = 0 ; classIndex < this.getClassLabels().length; classIndex++)
			if(className.equalsIgnoreCase(this.getClassLabels()[classIndex]))
				break;
		if(classIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + className + "' not found.");
			return false;
		}
		// Create loaded training data if it does not exist yet
		if(null == loadedTrainingData)
		{
			IJ.log("Initializing loaded data...");
			// Create instances
			ArrayList<Attribute> attributes = featureImages.getFeatureNamesAsAttributes();

			// Update list of names of loaded classes
			// (we assume the first two default class names)
			loadedClassNames = new ArrayList<String>();
			for(int i = 0; i < numOfClasses ; i ++)
				loadedClassNames.add(getClassLabels()[i]);
			attributes.add(new Attribute("class", loadedClassNames));
			loadedTrainingData = new Instances("segment", attributes, 1);

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes() - 1);
		}


		// Check all pixels different from black
		final int width = labelImage.getWidth();
		final int height = labelImage.getHeight();
		final ImageProcessor img = labelImage.getProcessor();
		int nl = 0;

		for(int x = 0 ; x < width ; x++)
			for(int y = 0 ; y < height; y++)
			{
				// White pixels are added to the class
				if(img.getPixelValue(x, y) > 0)
				{
					loadedTrainingData.add(featureImages.createInstance(x, y, 0, 0, classIndex));

					// increase number of instances for this class
					nl ++;
				}
			}


		IJ.log("Added " + nl + " instances of '" + className + "'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}


	/**
	 * Add instances from a labeled image. Each sample will be added to
	 * the class with the index value equal to the sample label. Labels
	 * that do not correspond to any class will be skipped.
	 *
	 * @param labelImage labeled image
	 * @param featureImages corresponding feature stack
	 * @return false if error
	 */
	public boolean addLabeledData(
			ImagePlus labelImage,
			FeatureImages featureImages )
	{

		IJ.showMessage("NOT IMPLEMENTED: addLabeledData");

		// Update features if necessary
		if(featureImages.getSize() < 2)
		{
			IJ.log("Creating feature stack...");
			featureImages.updateFeaturesMT(showFeatureImages);
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}

		// Create loaded training data if it does not exist yet
		if( null == loadedTrainingData )
		{
			IJ.log("Initializing loaded data...");

			// Create instances
			ArrayList<Attribute> attributes = featureImages.getFeatureNamesAsAttributes();

			// Update list of names of loaded classes
			// (we assume the first two default class names)
			loadedClassNames = new ArrayList<String>();
			for( int i = 0; i < numOfClasses ; i ++ )
				loadedClassNames.add( getClassLabels()[ i ] );
			attributes.add( new Attribute( "class", loadedClassNames ) );
			loadedTrainingData = new Instances( "segment", attributes, 1 );

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes() - 1);
		}

		// Check all pixels
		final int width = labelImage.getWidth();
		final int height = labelImage.getHeight();
		final ImageProcessor img = labelImage.getProcessor();
		int[] numSamples = new int[ numOfClasses ];

		for(int y = 0 ; y < height; y++)
			for(int x = 0 ; x < width ; x++)
			{
				int classIndex = (int) img.getf( x, y );

				if( classIndex >=0 && classIndex < numOfClasses )
				{
					loadedTrainingData.add(featureImages.createInstance(x, y, 0, 0, classIndex));
					numSamples[ classIndex ] ++;
				}
			}

		for( int i=0; i < numOfClasses; i ++ )
				IJ.log("Added " + numSamples[ i ] + " instances of '" + loadedClassNames.get(i) + "'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}
	/**
	 * Add instances from a labeled image in a random and balanced way.
	 * For convention, the label zero is used to define pixels with no class
	 * assigned. The rest of integer values correspond to the order of the
	 * classes (1 for the first class, 2 for the second class, etc.).
	 *
	 * @param labelImage labeled image (labels are positive integer or 0)
	 * @param featureImages corresponding feature stack
	 * @param numSamples number of samples to add of each class
	 * @return false if error
	 */
	public boolean addRandomBalancedLabeledData(
			ImageProcessor labelImage,
			FeatureImages featureImages,
			int numSamples )
	{

		IJ.showMessage("NOT IMPLEMENTED: addRandomBalancedLabeledData");


		// Update features if necessary
		if(featureImages.getSize() < 2)
		{
			IJ.log("Creating feature stack...");
			featureImages.updateFeaturesMT(showFeatureImages);
			//filterFeatureStackByList( this.featureNames, featureImages );
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}


		// Create loaded training data if it does not exist yet
		if( null == loadedTrainingData )
		{
			IJ.log("Initializing loaded data...");
			// Create instances
			ArrayList<Attribute> attributes = featureImages.getFeatureNamesAsAttributes();

			// Update list of names of loaded classes
			loadedClassNames = new ArrayList<String>();
			for(int i = 0; i < numOfClasses ; i ++)
				loadedClassNames.add(getClassLabels()[i]);

			attributes.add(new Attribute("class", loadedClassNames));
			loadedTrainingData = new Instances("segment", attributes, 1);

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes() - 1);
		}

		// Create lists of coordinates of pixels of each class
		ArrayList<Point>[] classCoordinates = new ArrayList[ numOfClasses ];
		for(int i = 0; i < numOfClasses ; i ++)
			classCoordinates[ i ] = new ArrayList<Point>();

		final int width = labelImage.getWidth();
		final int height = labelImage.getHeight();

		for(int y = 0 ; y < height; y++)
			for(int x = 0 ; x < width ; x++)
			{
				int classIndex = (int) labelImage.getf( x, y ) - 1;

				if( classIndex >=0 && classIndex < numOfClasses )
					classCoordinates[ classIndex ].add(new Point(x, y));
			}

		// Select random samples from each class
		Random rand = new Random();
		for( int i=0; i<numSamples; i++ )
		{
			for( int j = 0; j < numOfClasses ; j ++ )
			{
				if( !classCoordinates[ j ].isEmpty() )
				{
					int randomSample = rand.nextInt( classCoordinates[ j ].size() );

					loadedTrainingData.add(featureImages.createInstance(
							classCoordinates[j].get(randomSample).x,
							classCoordinates[j].get(randomSample).y,
							0, // TODO
							0,
							j));
				}
			}
		}

		for( int j = 0; j < numOfClasses ; j ++ )
			IJ.log("Added " + numSamples + " instances of '" + loadedClassNames.get( j ) +"'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}



	/**
	 * Add instances reading the pixel classes from a label image in a random
	 * and balanced way. The label image might contain more than 2 classes and
	 * its values need to be consecutive integers starting at 0 (0, 1, 2...)
	 * that correspond to the indexes of the class names provided by the user.
	 * Label values that do not correspond with any class index are skipped.
	 *
	 * @param labelImage label image with a different labeling per class
	 * @param featureStack corresponding feature stack
	 * @param classNames array with the corresponding names of the classes
	 * @param numSamples number of samples to add of each class
	 * @return false if error
	 */
	public boolean addRandomBalancedLabeledData(
			ImageProcessor labelImage,
			FeatureStack featureStack,
			String classNames[],
			int numSamples)
	{
		// Update features if necessary
		if( featureStack.getSize() < 2 )
		{
			IJ.log("Creating feature stack...");
			featureStack.updateFeaturesMT(showFeatureImages);
			//filterFeatureStackByList(this.featureNames, featureStack);
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}

		// Detect class indexes (in case they differ from the indexes in
		// getClassLabels()
		int classIndex[] = new int[ classNames.length ];
		for(int i = 0; i < classIndex.length; i++ )
		{
			for( classIndex[ i ] = 0 ; classIndex[ i ] < getClassLabels().length; classIndex[ i ]++ )
				if( classNames[i].equalsIgnoreCase( getClassLabels()[ classIndex[i] ] ) )
					break;
			if( classIndex[i] == getClassLabels().length)
			{
				IJ.log("Error: class named '" + classNames[ i ] + "' not found.");
				return false;
			}
		}

		// Create loaded training data if it does not exist yet
		if(null == loadedTrainingData)
		{
			IJ.log("Initializing loaded data...");
			// Create instances
			ArrayList<Attribute> attributes = new ArrayList<Attribute>();
			for (int i=1; i<=featureStack.getSize(); i++)
			{
				String attString = featureStack.getSliceLabel(i);
				attributes.add(new Attribute(attString));
			}

			if(featureStack.useNeighborhood())
				for (int i=0; i<8; i++)
				{
					IJ.log("Adding extra attribute original_neighbor_" + (i+1) + "...");
					attributes.add(new Attribute(new String("original_neighbor_" + (i+1))));
				}

			// Update list of names of loaded classes
			// (we assume the first two default class names)
			loadedClassNames = new ArrayList<String>();
			for(int i = 0; i < numOfClasses ; i ++)
				loadedClassNames.add(getClassLabels()[i]);

			attributes.add(new Attribute("class", loadedClassNames));
			loadedTrainingData = new Instances("segment", attributes, 1);

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes() - 1);
		}

		// Create lists of coordinates of pixels of all classes
		ArrayList<Point> classCoordinates[] = new ArrayList[ classNames.length ];
		final int width = labelImage.getWidth();
		final int height = labelImage.getHeight();

		for(int y = 0 ; y < height; y++)
			for(int x = 0 ; x < width ; x++)
			{
				// Add coordinates to corresponding class
				int val = (int) labelImage.getf( x, y );
				if( val >= 0 && val < classNames.length )
					classCoordinates[ val ].add( new Point( x, y ) );
			}

		// Select random samples from all classes
		Random rand = new Random();
		for( int i=0; i<numSamples; i++ )
		{
			for( int  j=0; j<classIndex.length; j++ )
			{
				int randomIndex = rand.nextInt( classCoordinates[ j ].size() );

				loadedTrainingData.add(featureStack.createInstance(
						classCoordinates[ j ].get( randomIndex ).x,
						classCoordinates[ j ].get( randomIndex ).y,
						classIndex[ j ] ) );
			}
		}

		for( int i=0; i<classNames.length; i++ )
			IJ.log( "Added " + numSamples + " instances of '"
					+ classNames[i] +"'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}

	/**
	 * Add instances to two classes from a label (binary) image in a random
	 * and balanced way (with repetition).
	 * White pixels will be added to the corresponding class 1 and
	 * black pixels will be added to class 2.
	 *
	 * @param labelImage binary image
	 * @param mask binary mask image to prevent some pixel to be selected (null if all pixels are eligible)
	 * @param featureStack corresponding feature stack
	 * @param whiteClassName name of the class which receives the white pixels
	 * @param blackClassName name of the class which receives the black pixels
	 * @param numSamples number of samples to add of each class
	 *
	 * @return false if error
	 */
	public boolean addRandomBalancedBinaryData(
			ImageProcessor labelImage,
			ImageProcessor mask,
			FeatureStack featureStack,
			String whiteClassName,
			String blackClassName,
			int numSamples)
	{
		// Update features if necessary
		if(featureStack.getSize() < 2)
		{
			IJ.log("Creating feature stack...");
			featureStack.updateFeaturesMT(showFeatureImages);
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}

		// Detect class indexes
		int whiteClassIndex = 0;
		for(whiteClassIndex = 0 ; whiteClassIndex < this.getClassLabels().length; whiteClassIndex++)
			if(whiteClassName.equalsIgnoreCase(this.getClassLabels()[whiteClassIndex]))
				break;
		if(whiteClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + whiteClassName + "' not found.");
			return false;
		}
		int blackClassIndex = 0;
		for(blackClassIndex = 0 ; blackClassIndex < this.getClassLabels().length; blackClassIndex++)
			if(blackClassName.equalsIgnoreCase(this.getClassLabels()[blackClassIndex]))
				break;
		if(blackClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + blackClassName + "' not found.");
			return false;
		}

		// Create loaded training data if it does not exist yet
		if(null == loadedTrainingData)
		{
			IJ.log("Initializing loaded data...");
			// Create instances
			ArrayList<Attribute> attributes = new ArrayList<Attribute>();
			for (int i=1; i<=featureStack.getSize(); i++)
			{
				String attString = featureStack.getSliceLabel(i);
				attributes.add(new Attribute(attString));
			}

			if(featureStack.useNeighborhood())
				for (int i=0; i<8; i++)
				{
					IJ.log("Adding extra attribute original_neighbor_" + (i+1) + "...");
					attributes.add(new Attribute(new String("original_neighbor_" + (i+1))));
				}

			// Update list of names of loaded classes
			// (we assume the first two default class names)
			loadedClassNames = new ArrayList<String>();
			for(int i = 0; i < numOfClasses ; i ++)
				loadedClassNames.add(getClassLabels()[i]);
			attributes.add(new Attribute("class", loadedClassNames));
			loadedTrainingData = new Instances("segment", attributes, 1);

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes() - 1);
		}

		// Create lists of coordinates of pixels of both classes
		ArrayList<Point> blackCoordinates = new ArrayList<Point>();
		ArrayList<Point> whiteCoordinates = new ArrayList<Point>();
		final int width = labelImage.getWidth();
		final int height = labelImage.getHeight();

		if( null != mask)
		{
			for(int y = 0 ; y < height; y++)
				for(int x = 0 ; x < width ; x++)
				{
					// White pixels are added to the class 1
					// and black to class 2
					if( mask.getPixelValue(x, y) > 0 )
					{
						if(labelImage.getPixelValue(x, y) > 0)
							whiteCoordinates.add(new Point(x, y));
						else
							blackCoordinates.add(new Point(x, y));
					}
				}
		}
		else
		{
			for(int y = 0 ; y < height; y++)
				for(int x = 0 ; x < width ; x++)
				{
					// White pixels are added to the class 1
					// and black to class 2
					if(labelImage.getPixelValue(x, y) > 0)
						whiteCoordinates.add(new Point(x, y));
					else
						blackCoordinates.add(new Point(x, y));

				}
		}

		// Select random samples from both classes
		Random rand = new Random();
		for(int i=0; i<numSamples; i++)
		{
			int randomBlack = rand.nextInt( blackCoordinates.size() );
			int randomWhite = rand.nextInt(whiteCoordinates.size());

			loadedTrainingData.add(featureStack.createInstance(blackCoordinates.get(randomBlack).x,
					blackCoordinates.get(randomBlack).y, blackClassIndex));
			loadedTrainingData.add(featureStack.createInstance(whiteCoordinates.get(randomWhite).x,
					whiteCoordinates.get(randomWhite).y, whiteClassIndex));
		}

		IJ.log("Added " + numSamples + " instances of '" + whiteClassName +"'.");
		IJ.log("Added " + numSamples + " instances of '" + blackClassName +"'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}


	/**
	 * Add instances to two classes from lists of coordinates in a random
	 * and balanced way.
	 * White pixels will be added to the corresponding class 1 and
	 * black pixels will be added to class 2.
	 *
	 * @param classPoints list of 3D coordinates to be used (x, y, slice)
	 * @param fsa feature stack array
	 * @param whiteClassName name of the class which receives the white pixels
	 * @param blackClassName name of the class which receives the black pixels
	 * @param numSamples number of samples to add of each class
	 *
	 * @return false if error
	 */
	public boolean addRandomBalancedBinaryData(
			List< Point3f >[] classPoints,
			FeatureImages fsa,
			String whiteClassName,
			String blackClassName,
			int numSamples)
	{

		// Detect class indexes
		int whiteClassIndex = 0;
		for(whiteClassIndex = 0 ; whiteClassIndex < this.getClassLabels().length; whiteClassIndex++)
			if(whiteClassName.equalsIgnoreCase(this.getClassLabels()[whiteClassIndex]))
				break;
		if(whiteClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + whiteClassName + "' not found.");
			return false;
		}
		int blackClassIndex = 0;
		for(blackClassIndex = 0 ; blackClassIndex < this.getClassLabels().length; blackClassIndex++)
			if(blackClassName.equalsIgnoreCase(this.getClassLabels()[blackClassIndex]))
				break;
		if(blackClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + blackClassName + "' not found.");
			return false;
		}

		// Create loaded training data if it does not exist yet
		if(null == loadedTrainingData)
		{
			IJ.log("Initializing loaded data...");
			// Create instances
			ArrayList<Attribute> attributes = new ArrayList<Attribute>();
			for (int i=1; i<=fsa.getNumFeatures(); i++)
			{
				String attString = fsa.getLabel( i );
				attributes.add(new Attribute(attString));
			}

			// Update list of names of loaded classes
			// (we assume the first two default class names)
			loadedClassNames = new ArrayList<String>();
			for(int i = 0; i < numOfClasses ; i ++)
				loadedClassNames.add(getClassLabels()[i]);
			attributes.add(new Attribute("class", loadedClassNames));
			loadedTrainingData = new Instances("segment", attributes, 1);

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes()-1);
		}

		// Select random samples from both classes
		Random rand = new Random();
		for(int i=0; i<numSamples; i++)
		{
			int randomBlack = rand.nextInt( classPoints[ 0 ].size() );
			int randomWhite = rand.nextInt( classPoints[ 1 ].size() );

			// add random black sample
			loadedTrainingData.add( featureImages.createInstance(
					(int) (classPoints[0].get(randomBlack).x),
					(int) (classPoints[0].get(randomBlack).y),
					(int) (classPoints[0].get(randomBlack).z),
					1,
					blackClassIndex) );

			// add random white sample
			loadedTrainingData.add( featureImages.createInstance(
					(int) (classPoints[0].get(randomBlack).x),
					(int) (classPoints[0].get(randomBlack).y),
					(int) (classPoints[0].get(randomBlack).z),
					1,
					whiteClassIndex) );
		}

		IJ.log("Added " + numSamples + " instances of '" + whiteClassName +"'.");
		IJ.log("Added " + numSamples + " instances of '" + blackClassName +"'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}

	/**
	 * Add instances to two classes from lists of coordinates.
	 *
	 * White pixels will be added to the corresponding class 1 and
	 * black pixels will be added to class 2.
	 *
	 * @param classPoints list of 3D coordinates to be used (x, y, slice)
	 * @param fsa feature stack array
	 * @param whiteClassName name of the class which receives the white pixels
	 * @param blackClassName name of the class which receives the black pixels
	 *
	 * @return false if error
	 */
	public boolean addBinaryData(
			List< Point3f >[] classPoints,
			FeatureImages fsa,
			String whiteClassName,
			String blackClassName)
	{

		// Detect class indexes
		int whiteClassIndex = 0;
		for(whiteClassIndex = 0 ; whiteClassIndex < this.getClassLabels().length; whiteClassIndex++)
			if(whiteClassName.equalsIgnoreCase(this.getClassLabels()[whiteClassIndex]))
				break;
		if(whiteClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + whiteClassName + "' not found.");
			return false;
		}
		int blackClassIndex = 0;
		for(blackClassIndex = 0 ; blackClassIndex < this.getClassLabels().length; blackClassIndex++)
			if(blackClassName.equalsIgnoreCase(this.getClassLabels()[blackClassIndex]))
				break;
		if(blackClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + blackClassName + "' not found.");
			return false;
		}

		// Create loaded training data if it does not exist yet
		if(null == loadedTrainingData)
		{
			IJ.log("Initializing loaded data...");
			// Create instances
			ArrayList<Attribute> attributes = new ArrayList<Attribute>();
			for (int i=1; i<=fsa.getNumFeatures(); i++)
			{
				String attString = fsa.getLabel( i );
				attributes.add(new Attribute(attString));
			}


			// Update list of names of loaded classes
			// (we assume the first two default class names)
			loadedClassNames = new ArrayList<String>();
			for(int i = 0; i < numOfClasses ; i ++)
				loadedClassNames.add(getClassLabels()[i]);
			attributes.add(new Attribute("class", loadedClassNames));
			loadedTrainingData = new Instances("segment", attributes, 1);

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes()-1);
		}

		// Add samples to both classes
		for(int i=0; i<classPoints[0].size(); i++)
			// add black sample
			loadedTrainingData.add( featureImages.createInstance(
					(int) (classPoints[0].get(i).x),
					(int) (classPoints[0].get(i).y),
					(int) (classPoints[0].get(i).z),
					1,
					blackClassIndex) );

		for(int i=0; i<classPoints[1].size(); i++)
			// add white sample
			loadedTrainingData.add( featureImages.createInstance(
					(int) (classPoints[0].get(i).x),
					(int) (classPoints[0].get(i).y),
					(int) (classPoints[0].get(i).z),
					1,
					whiteClassIndex) );


		IJ.log("Added " + classPoints[1].size() + " instances of '" + whiteClassName +"'.");
		IJ.log("Added " + classPoints[0].size() + " instances of '" + blackClassName +"'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}

	/**
	 * Add instances to two classes from lists of coordinates in a random
	 * and balanced way.
	 * White pixels will be added to the corresponding class 1 and
	 * black pixels will be added to class 2.
	 *
	 * @param classPoints list of 3D coordinates to be used (x, y, slice)
	 * @param fsa feature stack array
	 * @param weights weight image
	 * @param whiteClassName name of the class which receives the white pixels
	 * @param blackClassName name of the class which receives the black pixels
	 * @param numSamples number of samples to add of each class
	 *
	 * @return false if error
	 */
	public boolean addRandomBalancedBinaryData(
			List< Point3f >[] classPoints,
			FeatureImages fsa,
			ImagePlus weights,
			String whiteClassName,
			String blackClassName,
			int numSamples)
	{

		// Detect class indexes
		int whiteClassIndex = 0;
		for(whiteClassIndex = 0 ; whiteClassIndex < this.getClassLabels().length; whiteClassIndex++)
			if(whiteClassName.equalsIgnoreCase(this.getClassLabels()[whiteClassIndex]))
				break;
		if(whiteClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + whiteClassName + "' not found.");
			return false;
		}
		int blackClassIndex = 0;
		for(blackClassIndex = 0 ; blackClassIndex < this.getClassLabels().length; blackClassIndex++)
			if(blackClassName.equalsIgnoreCase(this.getClassLabels()[blackClassIndex]))
				break;
		if(blackClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + blackClassName + "' not found.");
			return false;
		}

		// Create loaded training data if it does not exist yet
		if(null == loadedTrainingData)
		{
			IJ.log("Initializing loaded data...");

			// Create instances

			ArrayList<Attribute> attributes = featureImages.getFeatureNamesAsAttributes();

			// Update list of names of loaded classes
			// (we assume the first two default class names)
			loadedClassNames = new ArrayList<String>();
			for(int i = 0; i < numOfClasses ; i ++)
				loadedClassNames.add(getClassLabels()[i]);
			attributes.add(new Attribute("class", loadedClassNames));
			loadedTrainingData = new Instances("segment", attributes, 1);

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes()-1);
		}

		final int width = weights.getWidth();

		// Select random samples from both classes
		Random rand = new Random();
		for(int i=0; i<numSamples; i++)
		{
			int randomBlack = rand.nextInt( classPoints[ 0 ].size() );
			int randomWhite = rand.nextInt( classPoints[ 1 ].size() );

			// add random black sample
			final int blackZ = (int) (classPoints[ 0 ].get(randomBlack).z);
			final int blackX = (int) (classPoints[ 0 ].get(randomBlack).x);
			final int blackY = (int) (classPoints[ 0 ].get(randomBlack).y);

			DenseInstance blackInstance = featureImages.createInstance(
					blackX,
					blackY,
					blackZ,
					0, // TODO
					blackClassIndex);

			blackInstance.setWeight( ((float[]) weights.getImageStack().getProcessor(
										blackZ + 1 ).getPixels())[ blackX + blackY * width ] );

			loadedTrainingData.add( blackInstance );

			// add random white sample
			final int whiteZ = (int) (classPoints[ 1 ].get(randomWhite).z);
			final int whiteX = (int) (classPoints[ 1 ].get(randomWhite).x);
			final int whiteY = (int) (classPoints[ 1 ].get(randomWhite).y);

			DenseInstance whiteInstance = featureImages.createInstance(
					whiteX,
					whiteY,
					whiteZ,
					0, // TODO
					whiteClassIndex);

			whiteInstance.setWeight(((float[]) weights.getImageStack().getProcessor(
					whiteZ + 1).getPixels())[whiteX + whiteY * width]);

			loadedTrainingData.add( whiteInstance );
		}

		IJ.log("Added " + numSamples + " instances of '" + whiteClassName +"'.");
		IJ.log("Added " + numSamples + " instances of '" + blackClassName +"'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}


	/**
	 * Add instances to two classes from a label (binary) image in a random
	 * and balanced way (with repetition).
	 * White pixels will be added to the corresponding class 1 and
	 * black pixels will be added to class 2.
	 *
	 * @param labelImage binary image
	 * @param mask binary mask image to prevent some pixel to be selected (null if all pixels are eligible)
	 * @param weights weight image
	 * @param featureStack corresponding feature stack
	 * @param whiteClassName name of the class which receives the white pixels
	 * @param blackClassName name of the class which receives the black pixels
	 * @param numSamples number of samples to add of each class

	 * @return false if error
	 */
	public boolean addRandomBalancedBinaryData(
			ImageProcessor labelImage,
			ImageProcessor mask,
			ImageProcessor weights,
			FeatureStack featureStack,
			String whiteClassName,
			String blackClassName,
			int numSamples)
	{
		// Update features if necessary
		if(featureStack.getSize() < 2)
		{
			IJ.log("Creating feature stack...");
			featureStack.updateFeaturesMT(showFeatureImages);
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}

		// Detect class indexes
		int whiteClassIndex = 0;
		for(whiteClassIndex = 0 ; whiteClassIndex < this.getClassLabels().length; whiteClassIndex++)
			if(whiteClassName.equalsIgnoreCase(this.getClassLabels()[whiteClassIndex]))
				break;
		if(whiteClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + whiteClassName + "' not found.");
			return false;
		}
		int blackClassIndex = 0;
		for(blackClassIndex = 0 ; blackClassIndex < this.getClassLabels().length; blackClassIndex++)
			if(blackClassName.equalsIgnoreCase(this.getClassLabels()[blackClassIndex]))
				break;
		if(blackClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + blackClassName + "' not found.");
			return false;
		}

		// Create loaded training data if it does not exist yet
		if(null == loadedTrainingData)
		{
			IJ.log("Initializing loaded data...");
			// Create instances
			ArrayList<Attribute> attributes = new ArrayList<Attribute>();
			for (int i=1; i<=featureStack.getSize(); i++)
			{
				String attString = featureStack.getSliceLabel(i);
				attributes.add(new Attribute(attString));
			}

			if(featureStack.useNeighborhood())
				for (int i=0; i<8; i++)
				{
					IJ.log("Adding extra attribute original_neighbor_" + (i+1) + "...");
					attributes.add(new Attribute(new String("original_neighbor_" + (i+1))));
				}

			// Update list of names of loaded classes
			// (we assume the first two default class names)
			loadedClassNames = new ArrayList<String>();
			for(int i = 0; i < numOfClasses ; i ++)
				loadedClassNames.add(getClassLabels()[i]);
			attributes.add(new Attribute("class", loadedClassNames));
			loadedTrainingData = new Instances("segment", attributes, 1);

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes() - 1);
		}

		// Create lists of coordinates of pixels of both classes
		ArrayList<Point> blackCoordinates = new ArrayList<Point>();
		ArrayList<Point> whiteCoordinates = new ArrayList<Point>();
		final int width = labelImage.getWidth();
		final int height = labelImage.getHeight();

		if( null != mask )
		{
			for(int y = 0 ; y < height; y++)
				for(int x = 0 ; x < width ; x++)
				{
					// White pixels are added to the class 1
					// and black to class 2
					if(mask.getPixelValue(x, y) > 0)
					{
						if(labelImage.getPixelValue(x, y) > 0)
							whiteCoordinates.add(new Point(x, y));
						else
							blackCoordinates.add(new Point(x, y));
					}
				}
		}
		else
		{
			for(int y = 0 ; y < height; y++)
				for(int x = 0 ; x < width ; x++)
				{
					// White pixels are added to the class 1
					// and black to class 2
					if(labelImage.getPixelValue(x, y) > 0)
						whiteCoordinates.add(new Point(x, y));
					else
						blackCoordinates.add(new Point(x, y));
				}
		}
		// Select random samples from both classes
		Random rand = new Random();
		for(int i=0; i<numSamples; i++)
		{
			int randomBlack = rand.nextInt( blackCoordinates.size() );
			int randomWhite = rand.nextInt( whiteCoordinates.size() );


			DenseInstance blackSample = featureStack.createInstance( blackCoordinates.get(randomBlack).x,
					blackCoordinates.get(randomBlack).y, blackClassIndex);
			blackSample.setWeight( weights.getPixelValue(  	blackCoordinates.get(randomBlack).x,
														 	blackCoordinates.get(randomBlack).y) );
			loadedTrainingData.add(blackSample);

			DenseInstance whiteSample = featureStack.createInstance(whiteCoordinates.get(randomWhite).x,
					whiteCoordinates.get(randomWhite).y, whiteClassIndex);

			whiteSample.setWeight(weights.getPixelValue(whiteCoordinates.get(randomWhite).x,
					whiteCoordinates.get(randomWhite).y));
			loadedTrainingData.add(whiteSample);
		}

		IJ.log("Added " + numSamples + " instances of '" + whiteClassName +"'.");
		IJ.log("Added " + numSamples + " instances of '" + blackClassName +"'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}

	/**
	 * Add instances defined by a label (binary) image in a random
	 * way.
	 * White pixels (with intensity values larger than 0) will be added to the
	 * corresponding class 1 (defined by whiteClassName)
	 *
	 * @param labelImage binary image
	 * @param featureStack corresponding feature stack
	 * @param whiteClassName name of the class which receives the white pixels
	 * @param numSamples number of samples to add of each class
	 * @return false if error
	 */
	public boolean addRandomData(
			ImagePlus labelImage,
			FeatureStack featureStack,
			String whiteClassName,
			int numSamples)
	{
		// Update features if necessary
		if(featureStack.getSize() < 2)
		{
			IJ.log("Creating feature stack...");
			featureStack.updateFeaturesMT(showFeatureImages);
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}

		// Detect class indexes
		int whiteClassIndex = 0;
		for(whiteClassIndex = 0 ; whiteClassIndex < this.getClassLabels().length; whiteClassIndex++)
			if(whiteClassName.equalsIgnoreCase(this.getClassLabels()[whiteClassIndex]))
				break;
		if(whiteClassIndex == this.getClassLabels().length)
		{
			IJ.log("Error: class named '" + whiteClassName + "' not found.");
			return false;
		}

		// Create loaded training data if it does not exist yet
		if(null == loadedTrainingData)
		{
			IJ.log("Initializing loaded data...");
			// Create instances
			ArrayList<Attribute> attributes = new ArrayList<Attribute>();
			for (int i=1; i<=featureStack.getSize(); i++)
			{
				String attString = featureStack.getSliceLabel(i);
				attributes.add(new Attribute(attString));
			}

			if(featureStack.useNeighborhood())
				for (int i=0; i<8; i++)
				{
					IJ.log("Adding extra attribute original_neighbor_" + (i+1) + "...");
					attributes.add(new Attribute(new String("original_neighbor_" + (i+1))));
				}

			// Update list of names of loaded classes
			// (we assume the first two default class names)
			loadedClassNames = new ArrayList<String>();
			for(int i = 0; i < numOfClasses ; i ++)
				loadedClassNames.add(getClassLabels()[i]);
			attributes.add(new Attribute("class", loadedClassNames));
			loadedTrainingData = new Instances("segment", attributes, 1);

			loadedTrainingData.setClassIndex(loadedTrainingData.numAttributes() - 1);
		}

		// Create lists of coordinates of pixels of white class
		ArrayList<Point> whiteCoordinates = new ArrayList<Point>();
		final int width = labelImage.getWidth();
		final int height = labelImage.getHeight();
		final ImageProcessor img = labelImage.getProcessor();

		for(int y = 0 ; y < height; y++)
			for(int x = 0 ; x < width ; x++)
			{
				// White pixels are added to the white class
				if(img.getPixelValue(x, y) > 0)
					whiteCoordinates.add(new Point(x, y));
			}

		// Select random samples from white class
		Random rand = new Random();
		for(int i=0; i<numSamples; i++)
		{
			int randomWhite = rand.nextInt(whiteCoordinates.size());

			loadedTrainingData.add(featureStack.createInstance(whiteCoordinates.get(randomWhite).x,
					whiteCoordinates.get(randomWhite).y, whiteClassIndex));
		}

		IJ.log("Added " + numSamples + " instances of '" + whiteClassName +"'.");

		IJ.log("Training dataset updated ("+ loadedTrainingData.numInstances() +
				" instances, " + loadedTrainingData.numAttributes() +
				" attributes, " + loadedTrainingData.numClasses() + " classes).");

		return true;
	}


	/**
	 * Add training data from input and labeled images in a
	 * random and balanced way (same number of samples per class).
	 * Input and labeled images can be 2D or stacks and their
	 * sizes must match. For convention, the label zero is used to define pixels
	 * with no class assigned. The rest of integer values correspond to the
	 * order of the classes (1 for the first class, 2 for the second class,
	 * etc.).
	 *
	 * @param inputImage input grayscale image
	 * @param labelImage labeled image (labeled values are positive integer or 0)
	 * @param numSamples number of samples to pick for each class
	 * @return false if error
	 */
	public boolean addRandomBalancedLabeledData(
			ImagePlus inputImage,
			ImagePlus labelImage,
			int numSamples )
	{

		// Check sizes
		if(labelImage.getWidth() != inputImage.getWidth()
				|| labelImage.getHeight() != inputImage.getHeight()
				|| labelImage.getImageStackSize() != inputImage.getImageStackSize())
		{
			IJ.log("Error: label and training image sizes do not fit.");
			return false;
		}

		final ImageStack inputSlices = inputImage.getImageStack();
		final ImageStack labelSlices = labelImage.getImageStack();

		for(int i=1; i <= inputSlices.getSize(); i++)
		{
			// Create feature stack for the slice
			final FeatureStack featureStack = new FeatureStack(new ImagePlus("slice " + i, inputSlices.getProcessor( i ) ) );
			featureStack.setEnabledFeatures( enabledFeatures );
			featureStack.setMembranePatchSize(membranePatchSize);
			featureStack.setMembraneSize(this.membraneThickness);
			featureStack.setMaximumSigma(this.maximumFeatureScale);
			featureStack.setMinimumSigma(this.minimumSigma);
			IJ.log("Creating feature stack for slice " + i + "...");
			featureStack.updateFeaturesMT(showFeatureImages);
			IJ.log("Feature stack is now updated.");


			// add labeled data based on the labeled image
			/*
			if(!addRandomBalancedLabeledData(labelSlices.getProcessor(i), featureStack, numSamples))
			{
				IJ.log("Error while loading label data from slice " + i);
				return false;
			}*/
		}
		return true;
	}

	/**
	 * Add training data from input and labeled images in a
	 * random and balanced way (same number of samples per class).
	 * Input and labeled images can be 2D or stacks and their
	 * sizes must match. The label values must correspond with the indexes
	 * of the class names provided by the user.
	 *
	 * @param inputImage input grayscale image
	 * @param labelImage labeled image (labeled values are positive integer or 0)
	 * @param classNames array with the corresponding class names
	 * @param numSamples number of samples to pick for each class
	 * @return false if error
	 */
	public boolean addRandomBalancedLabeledData(
			ImagePlus inputImage,
			ImagePlus labelImage,
			String classNames[],
			int numSamples )
	{

		// Check sizes
		if(labelImage.getWidth() != inputImage.getWidth()
				|| labelImage.getHeight() != inputImage.getHeight()
				|| labelImage.getImageStackSize() != inputImage.getImageStackSize())
		{
			IJ.log("Error: label and training image sizes do not fit.");
			return false;
		}

		final ImageStack inputSlices = inputImage.getImageStack();
		final ImageStack labelSlices = labelImage.getImageStack();

		for(int i=1; i <= inputSlices.getSize(); i++)
		{
			// Create feature stack for the slice
			final FeatureStack featureStack =
					new FeatureStack( new ImagePlus( "slice " + i,
							inputSlices.getProcessor( i ) ) );
			featureStack.setEnabledFeatures(enabledFeatures);
			featureStack.setMembranePatchSize(membranePatchSize);
			featureStack.setMembraneSize(this.membraneThickness);
			featureStack.setMaximumSigma(this.maximumFeatureScale);
			featureStack.setMinimumSigma(this.minimumSigma);
			IJ.log("Creating feature stack for slice " + i + "...");
			featureStack.updateFeaturesMT(showFeatureImages);
			IJ.log("Feature stack is now updated.");

			// add labeled data based on the labeled image
			if( !addRandomBalancedLabeledData(
					labelSlices.getProcessor(i), featureStack, classNames,
					numSamples))
			{
				IJ.log( "Error while loading label data from slice " + i );
				return false;
			}
		}
		return true;
	}

	/**
	 * Add binary training data from input and label images in a
	 * random and balanced way (same number of samples per class).
	 * Input and label images can be 2D or stacks and their
	 * sizes must match.
	 *
	 * @param inputImage input grayscale image
	 * @param labelImage binary label image
	 * @param whiteClassName class name for the white pixels
	 * @param blackClassName class name for the black pixels
	 * @param numSamples number of samples to pick for each class
	 * @param mask mask to prevent some pixel to be selected (null if all pixels are eligible)
	 * @return false if error
	 */
	public boolean addRandomBalancedBinaryData(
			ImagePlus inputImage,
			ImagePlus labelImage,
			String whiteClassName,
			String blackClassName,
			int numSamples,
			ImagePlus mask)
	{

		// Check sizes
		if(labelImage.getWidth() != inputImage.getWidth()
				|| labelImage.getHeight() != inputImage.getHeight()
				|| labelImage.getImageStackSize() != inputImage.getImageStackSize())
		{
			IJ.log("Error: label and training image sizes do not fit.");
			return false;
		}

		final ImageStack inputSlices = inputImage.getImageStack();
		final ImageStack labelSlices = labelImage.getImageStack();

		for(int i=1; i <= inputSlices.getSize(); i++)
		{

			// Process label pixels
			final ImagePlus labelIP = new ImagePlus ("labels", labelSlices.getProcessor(i).duplicate());
			// Make sure it's binary
			labelIP.getProcessor().autoThreshold();

			final FeatureStack featureStack = new FeatureStack(new ImagePlus("slice " + i, inputSlices.getProcessor(i)));
			featureStack.setEnabledFeatures(enabledFeatures);
			featureStack.setMembranePatchSize(membranePatchSize);
			featureStack.setMembraneSize(this.membraneThickness);
			featureStack.setMaximumSigma(this.maximumFeatureScale);
			featureStack.setMinimumSigma(this.minimumSigma);
			IJ.log("Creating feature stack for slice " + i + "...");
			featureStack.updateFeaturesMT(showFeatureImages);
			IJ.log("Feature stack is now updated.");

			if(!addRandomBalancedBinaryData(labelIP.getProcessor(),
					null == mask ? null : mask.getImageStack().getProcessor(i),
					featureStack, whiteClassName, blackClassName, numSamples))
			{
				IJ.log("Error while loading binary label data from slice " + i);
				return false;
			}
		}
		return true;
	}

	/**
	 * Add binary training data from input and label images in a
	 * random and balanced way (same number of samples per class).
	 * Input and label images can be 2D or stacks and their
	 * sizes must match.
	 *
	 * @param inputImage input grayscale image
	 * @param labelImage binary label image
	 * @param whiteClassName class name for the white pixels
	 * @param blackClassName class name for the black pixels
	 * @param numSamples number of samples to pick for each class
	 * @param mask mask to prevent some pixel to be selected (null if all pixels are eligible)
	 * @param weights image containing the weight of each sample
	 * @return false if error
	 */
	public boolean addRandomBalancedBinaryData(
			ImagePlus inputImage,
			ImagePlus labelImage,
			String whiteClassName,
			String blackClassName,
			int numSamples,
			ImagePlus mask,
			ImagePlus weights)
	{

		// Check sizes
		if(labelImage.getWidth() != inputImage.getWidth()
				|| labelImage.getHeight() != inputImage.getHeight()
				|| labelImage.getImageStackSize() != inputImage.getImageStackSize())
		{
			IJ.log("Error: label and training image sizes do not fit.");
			return false;
		}

		final ImageStack inputSlices = inputImage.getImageStack();
		final ImageStack labelSlices = labelImage.getImageStack();

		for(int i=1; i <= inputSlices.getSize(); i++)
		{

			// Process label pixels
			final ImagePlus labelIP = new ImagePlus ("labels", labelSlices.getProcessor(i).duplicate());
			// Make sure it's binary
			labelIP.getProcessor().autoThreshold();

			final FeatureStack featureStack = new FeatureStack(new ImagePlus("slice " + i, inputSlices.getProcessor(i)));
			featureStack.setEnabledFeatures(enabledFeatures);
			featureStack.setMembranePatchSize(membranePatchSize);
			featureStack.setMembraneSize(this.membraneThickness);
			featureStack.setMaximumSigma(this.maximumFeatureScale);
			featureStack.setMinimumSigma(this.minimumSigma);
			IJ.log("Creating feature stack for slice " + i + "...");
			featureStack.updateFeaturesMT(showFeatureImages);
			IJ.log("Feature stack is now updated.");


			if(!addRandomBalancedBinaryData(labelIP.getProcessor(),
					null == mask ? null : mask.getImageStack().getProcessor(i),
					weights.getImageStack().getProcessor(i),
					featureStack, whiteClassName, blackClassName, numSamples))
			{
				IJ.log("Error while loading binary label data from slice " + i);
				return false;
			}
		}
		return true;
	}

	/**
	 * Add training data from input and a binary label image in a
	 * random way.
	 * Input and label images can be 2D or stacks and their
	 * sizes must match. The data will be added to the defined white class.
	 *
	 * @param inputImage input grayscale image
	 * @param labelImage binary label image (labels in white)
	 * @param whiteClassName class name for the white pixels
	 * @param numSamples number of samples to pick for each class
	 * @return false if error
	 */
	public boolean addRandomData(
			ImagePlus inputImage,
			ImagePlus labelImage,
			String whiteClassName,
			int numSamples)
	{

		// Check sizes
		if(labelImage.getWidth() != inputImage.getWidth()
				|| labelImage.getHeight() != inputImage.getHeight()
				|| labelImage.getImageStackSize() != inputImage.getImageStackSize())
		{
			IJ.log("Error: label and training image sizes do not fit.");
			return false;
		}

		final ImageStack inputSlices = inputImage.getImageStack();
		final ImageStack labelSlices = labelImage.getImageStack();

		for(int i=1; i <= inputSlices.getSize(); i++)
		{

			// Process label pixels
			final ImagePlus labelIP = new ImagePlus ("labels", labelSlices.getProcessor(i).duplicate());
			// Make sure it's binary
			labelIP.getProcessor().autoThreshold();

			final FeatureStack featureStack = new FeatureStack(new ImagePlus("slice " + i, inputSlices.getProcessor(i)));
			featureStack.setEnabledFeatures(enabledFeatures);
			featureStack.setMembranePatchSize(membranePatchSize);
			featureStack.setMembraneSize(this.membraneThickness);
			featureStack.setMaximumSigma(this.maximumFeatureScale);
			featureStack.setMinimumSigma(this.minimumSigma);
			IJ.log("Creating feature stack for slice " + i + "...");
			featureStack.updateFeaturesMT(showFeatureImages);
			IJ.log("Feature stack is now updated.");


			if(!addRandomData(labelIP, featureStack, whiteClassName, numSamples))
			{
				IJ.log("Error while loading binary label data from slice " + i);
				return false;
			}
		}
		return true;
	}


	/**
	 * Add binary training data from input and label images in a
	 * random and balanced way (same number of samples per class).
	 * The features will be created out of a list of filters.
	 * Input and label images can be 2D or stacks and their
	 * sizes must match.
	 *
	 * @param inputImage input grayscale image
	 * @param labelImage binary label image
	 * @param filters stack of filters to create features
	 * @param whiteClassName class name for the white pixels
	 * @param blackClassName class name for the black pixels
	 * @param numSamples number of samples to pick for each class
	 * @return false if error
	 */
	public boolean addRandomBalancedBinaryData(
			ImagePlus inputImage,
			ImagePlus labelImage,
			ImagePlus filters,
			String whiteClassName,
			String blackClassName,
			int numSamples)
	{

		// Check sizes
		if(labelImage.getWidth() != inputImage.getWidth()
				|| labelImage.getHeight() != inputImage.getHeight()
				|| labelImage.getImageStackSize() != inputImage.getImageStackSize())
		{
			IJ.log("Error: label and training image sizes do not fit.");
			return false;
		}

		final ImageStack inputSlices = inputImage.getImageStack();
		final ImageStack labelSlices = labelImage.getImageStack();

		for(int i=1; i <= inputSlices.getSize(); i++)
		{

			// Process label pixels
			final ImagePlus labelIP = new ImagePlus ("labels", labelSlices.getProcessor(i).duplicate());
			// Make sure it's binary
			labelIP.getProcessor().autoThreshold();

			final FeatureStack featureStack = new FeatureStack(new ImagePlus("slice " + i, inputSlices.getProcessor(i)));
			featureStack.addFeaturesMT(filters);


			/*
			if(!addRandomBalancedBinaryData(labelIP.getProcessor(), featureStack, whiteClassName, blackClassName,
					numSamples))
			{
				IJ.log("Error while loading binary label data from slice " + i);
				return false;
			}
			*/
		}
		return true;
	}


	/**
	 * Add binary training data from input and label images.
	 * The features will be created out of a list of filters.
	 * Input and label images can be 2D or stacks and their
	 * sizes must match.
	 *
	 * @param inputImage input grayscale image
	 * @param labelImage binary label image
	 * @param filters stack of filters to create features
	 * @param whiteClassName class name for the white pixels
	 * @param blackClassName class name for the black pixels
	 * @return false if error
	 */
	public boolean addBinaryData(
			ImagePlus inputImage,
			ImagePlus labelImage,
			ImagePlus filters,
			String whiteClassName,
			String blackClassName)
	{

		// Check sizes
		if(labelImage.getWidth() != inputImage.getWidth()
				|| labelImage.getHeight() != inputImage.getHeight()
				|| labelImage.getImageStackSize() != inputImage.getImageStackSize())
		{
			IJ.log("Error: label and training image sizes do not fit.");
			return false;
		}

		final ImageStack inputSlices = inputImage.getImageStack();
		final ImageStack labelSlices = labelImage.getImageStack();

		for(int i=1; i <= inputSlices.getSize(); i++)
		{

			// Process label pixels
			final ImagePlus labelIP = new ImagePlus ("labels", labelSlices.getProcessor(i).duplicate());
			// Make sure it's binary
			labelIP.getProcessor().autoThreshold();

			final FeatureStack featureStack = new FeatureStack(new ImagePlus("slice " + i, inputSlices.getProcessor(i)));
			featureStack.addFeaturesMT(filters);


			/*
			if(!this.addBinaryData(labelIP, featureStack, whiteClassName, blackClassName))
			{
				IJ.log("Error while loading binary label data from slice " + i);
				return false;
			}
			*/
		}
		return true;
	}


	/**
	 * Add eroded version of label image as binary data
	 *
	 * @param labelImage binary label image
	 * @param whiteClassName class name for the white pixels
	 * @param blackClassName class name for the black pixels
	 * @return false if error
	 */
	public boolean addErodedBinaryData(
			ImagePlus labelImage,
			int n,
			String whiteClassName,
			String blackClassName)
	{
		return true;
		/*
		// Update features if necessary
		if(featureImages.get(n).getSize() < 2)
		{
			IJ.log("Creating feature stack...");
			featureImages.get(n).updateFeaturesMT(showFeatureImages);
			filterFeatureStackByList();
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}

		if(labelImage.getWidth() != this.trainingImage.getWidth()
				|| labelImage.getHeight() != this.trainingImage.getHeight())
		{
			IJ.log("Error: label and training image sizes do not fit.");
			return false;
		}

		// Process white pixels
		final ImagePlus whiteIP = new ImagePlus ("white", labelImage.getProcessor().duplicate());
		IJ.run(whiteIP, "Erode","");
		// Add skeleton to white class
		if(!this.addBinaryData(whiteIP, featureImages.get(n), whiteClassName))
		{
			IJ.log("Error while loading white class center-lines data.");
			return false;
		}

		// Process black pixels
		final ImagePlus blackIP = new ImagePlus ("black", labelImage.getProcessor().duplicate());
		IJ.run(blackIP, "Invert","");
		IJ.run(blackIP, "Erode","");
		// Add skeleton to white class
		if(!this.addBinaryData(blackIP, featureImages.get(n), blackClassName))
		{
			IJ.log("Error while loading black class center-lines data.");
			return false;
		}
		return true;
		*/
	}



	/**
	 * Classify a list of images in a concurrent way
	 * @param images list of images to classify
	 * @param dataInfo empty set of instances containing the data structure (attributes and classes)
	 * @param classifier classifier to use
	 * @param counter counter used to display the progress in the tool bar
	 * @param probabilityMaps flag to calculate probabilities or binary results
	 * @return classification result
	 */
	public Callable< ArrayList< ImagePlus >> classifyListOfImages(
			final ArrayList<ImagePlus> images,
			final Instances dataInfo,
			final AbstractClassifier classifier,
			final AtomicInteger counter,
			final boolean probabilityMaps)
	{
		if (Thread.currentThread().isInterrupted())
			return null;

		return new Callable < ArrayList< ImagePlus >>(){
			@Override
			public ArrayList < ImagePlus > call()
			{
				ArrayList < ImagePlus > result = new ArrayList < ImagePlus >();

				for(ImagePlus image : images )
				{
					// Create feature stack for the image
					IJ.showStatus("Creating features...");
					IJ.log("Creating features of slice " + image.getTitle() + ", size = " + image.getWidth() + "x" + image.getHeight() + "...");
					final FeatureStack sliceFeatures = new FeatureStack( image );
					// Use the same features as the current classifier
					sliceFeatures.setEnabledFeatures(enabledFeatures);
					sliceFeatures.setMaximumSigma(maximumFeatureScale);
					sliceFeatures.setMinimumSigma(minimumSigma);
					sliceFeatures.setMembranePatchSize(membranePatchSize);
					sliceFeatures.setMembraneSize(membraneThickness);
					if(!sliceFeatures.updateFeaturesST())
					{
						IJ.log("Classifier execution was interrupted.");
						return null;
					}

					final int width = image.getWidth();
					final int height = image.getHeight();
					final int numClasses = dataInfo.numClasses();

					ImageStack classificationResult = new ImageStack(width, height);

					final int numInstances = width * height;

					final double[][] probArray;

					if (probabilityMaps)
						probArray = new double[numClasses][numInstances];
					else
						probArray = new double[1][numInstances];

					IJ.log("Classifying slice " + image.getTitle() + "...");

					// auxiliary array to be filled for each instance
					final int extra = sliceFeatures.useNeighborhood() ? 8 : 0;
					final double[] values =
							new double[ sliceFeatures.getSize() + 1 + extra ];
					// create empty reusable instance
					final ReusableDenseInstance ins =
							new ReusableDenseInstance( 1.0, values );
					ins.setDataset( dataInfo );

					for (int i=0; i<numInstances; i++)

					for (int x=0; x<width; x++)
						for(int y=0; y<height; y++)
						{
							try{

								if (0 == (x+y*width) % 4000)
								{
									if (Thread.currentThread().isInterrupted())
										return null;
									counter.addAndGet(4000);
								}

								sliceFeatures.setInstance(
										x, y, 0, ins, values );

								if (probabilityMaps)
								{
									double[] prob = classifier.distributionForInstance( ins );
									for(int k = 0 ; k < numClasses; k++)
									{
										probArray[k][x+y*width] = prob[ k ];
									}
								}
								else
								{
									probArray[0][ x+y*width ] = classifier.classifyInstance( ins );
								}

							}catch(Exception e){

								IJ.showMessage("Could not apply Classifier!");
								e.printStackTrace();
								return null;
							}
						}

					if( probabilityMaps )
					{
						for(int k = 0 ; k < numClasses; k++)
							classificationResult.addSlice("class-" + (k+1), new FloatProcessor(width, height, probArray[k]) );
					}
					else
						classificationResult.addSlice("result", new FloatProcessor(width, height, probArray[0]) );

					result.add( new ImagePlus("classified-image-"+image.getTitle(), classificationResult) );
				}
				return result;
			}
		};
	}

	/**
	 * Get loaded (or accumulated) training instances
	 *
	 * @return loaded/accumulated training instances
	 */
	public Instances getTrainingInstances()
	{
		return this.loadedTrainingData;
	}




	/**
	 * Add the current ROI to a specific class and slice.
	 *
	 * @param classNum string representing the class index
	 * @param nSlice string representing the slice number
	 */
	public static void addTrace(
			String classNum,
			String nSlice,
			String nTimePoint)
	{
		final ImageWindow iw = WindowManager.getCurrentImage().getWindow();
		if( iw instanceof CustomWindow )
		{
			final CustomWindow win = (CustomWindow) iw;
			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();
			final Roi roi = win.getDisplayImage().getRoi();
			wekaSegmentation.addExample(Integer.parseInt(classNum),
					roi,
					Integer.parseInt(nSlice)-1,
					Integer.parseInt(nTimePoint)-1);
			win.getDisplayImage().killRoi();
			win.drawExamples();
			win.updateExampleLists();
		}
	}

	/**
	 * Delete a specific ROI from the list of a specific class and slice
	 *
	 * @param classNum string representing the class index
	 * @param nSlice string representing the slice number
	 * @param index string representing the index of the trace to remove
	 */
	public void deleteTrace(
			String classNum,
			String nSlice,
			String nTimePoint,
			String index)
	{
		//final ImageWindow iw = WindowManager.getCurrentImage().getWindow();

		//if( iw instanceof CustomWindow )
		//{
		//	final CustomWindow win = (CustomWindow) iw;

			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();
			wekaSegmentation.deleteExample(Integer.parseInt(classNum),
					Integer.parseInt(nSlice),
					Integer.parseInt(nTimePoint),
					Integer.parseInt(index));
			win.getDisplayImage().killRoi();
			win.drawExamples();
			win.updateExampleLists();
		//}
	}


/**
	 * Apply current classifier to a set of feature vectors (given in a feature
	 * stack array). The classification if performed in a multi-threaded way
	 * using as many threads as defined by the user.
	 *
	 * @param featureImages feature stack array
	 * @param numThreads The number of threads to use. Set to zero for auto-detection.
	 * @param probabilityMaps probability flag. Tue: probability maps are calculated, false: binary classification
	 * @return result image containing the probability maps or the binary classification
	 */
	public void applyClassifierOld(
			final Region5D region5D,
			final boolean updateFeatureImages,
			final int numThreads,
			final boolean probabilityMaps)
	{

		// TODO: why should the training image be set globally?

		logger.info("Applying classifier...");

		long memBefore = IJ.currentMemory();
		IJ.log("Memory usage before [MB] :"
				+ memBefore/1000000L + "/" + IJ.maxMemory()/1000000L);


		ImagePlus imageToClassify = null;

		if ( region5D != null )
		{
			if (trainingImage.getImageStack() instanceof VirtualStackOfStacks)
			{
				VirtualStackOfStacks vss = (VirtualStackOfStacks) trainingImage.getStack();
				imageToClassify = vss.getDataCube(region5D, 0, numThreads);
			}
			else
			{
				IJ.showMessage("ERROR: no VSS");
				return;
			}
		}
		else
		{
			imageToClassify = trainingImage; // classify whole image
		}

		if ( updateFeatureImages )
		{
			imageToClassify.show();
			featureImages.setNumFeatureScales(nFeatureScales);
			featureImages.setOriginalImage(imageToClassify);
			featureImages.setEnabledFeatures(enabledFeatures);
			featureImages.updateFeaturesMT( showFeatureImages );
		}

		int nx = imageToClassify.getWidth();
		int ny = imageToClassify.getHeight();
		int nz = imageToClassify.getNSlices();

		final int border = getFeatureImagesBorderSize();

		ArrayList<String> classNames = null;
		if(null != loadedClassNames)
			classNames = loadedClassNames;
		else
		{
			classNames = getClassNamesAsArrayList();

			//for(int i = 0; i < numOfClasses; i++)
			//		if(!classNames.contains(getClassNames()[i]))
			//			classNames.add(getClassNames()[i]);
		}

		// create instances information (each instance needs a pointer to this)
		//
		Instances dataInfo = new Instances("segment", getAttributes(), 1);
		dataInfo.setClassIndex(dataInfo.numAttributes()-1);

		// number of classes
		final int numClasses = classNames.size();

		// total number of instances (i.e. feature vectors)
		final int numInstances = nx * ny * nz;

		// number of channels of the result image
		final int numChannels  = (probabilityMaps ? numClasses : 1);
		// number of slices of the result image
		//final int numSlices    = (numChannels*numInstances)/(trainingImage.getWidth()*trainingImage.getHeight());
		final int numSlices  = numChannels * nz; // TODO

		final long start = System.currentTimeMillis();

		exe = Executors.newFixedThreadPool(numThreads);
		final byte[][][] results = new byte[numThreads][][];
		final int partialSize = numInstances / numThreads;
		Future<byte[][]>[] fu = new Future[numThreads];

		final AtomicInteger counter = new AtomicInteger();

		for(int i = 0; i < numThreads; i++)
		{
			if (Thread.currentThread().isInterrupted())
				return;

			int first = i*partialSize;
			int size = (i == numThreads - 1) ? numInstances - i*partialSize : partialSize;

			AbstractClassifier classifierCopy = null;
			try {
				// The Weka random forest classifiers do not need to be duplicated on each thread
				// (that saves much memory)
				if( classifier instanceof FastRandomForest || classifier instanceof RandomForest )
					classifierCopy = classifier;
				else
					classifierCopy = (AbstractClassifier) (AbstractClassifier.makeCopy( classifier ));
			} catch (Exception e) {
				IJ.log("Error: classifier could not be copied to classify in a multi-thread way.");
				e.printStackTrace();
			}

			fu[i] = exe.submit( classifyInstances( featureImages, region5D.t + 1, dataInfo, first, size, border, classifierCopy, counter, probabilityMaps ) );
		}

		ScheduledExecutorService monitor = Executors.newScheduledThreadPool(1);
		ScheduledFuture task = monitor.scheduleWithFixedDelay(new Runnable() {
			@Override
			public void run()
			{
				IJ.showProgress(counter.get(), numInstances);
			}
		}, 0, 1, TimeUnit.SECONDS);

		// Join threads
		try {
			for(int i = 0; i < numThreads; i++)
				results[i] = fu[i].get();
		} catch (InterruptedException e) {
			//e.printStackTrace();
			return;
		} catch (ExecutionException e) {
			e.printStackTrace();
			return;
		} catch ( OutOfMemoryError err ) {
			IJ.log( "ERROR: applyClassifier run out of memory. Please, "
					+ "use a smaller input image or fewer features." );
			err.printStackTrace();
			return;
		} finally {
			exe.shutdown();
			task.cancel(true);
			monitor.shutdownNow();
			IJ.showProgress(1);
		}

		// Create final array
		byte[][] classificationResult = new byte[numChannels][numInstances];

		for(int i = 0; i < numThreads; i++)
			for (int c = 0; c < numChannels; c++)
				System.arraycopy(results[i][c], 0, classificationResult[c], i*partialSize, results[i][c].length);

		IJ.showProgress(1.0);
		final long end = System.currentTimeMillis();
		IJ.log("Classification took: " + (end - start) + "ms");

		long memAfter = IJ.currentMemory();
		long memDiff = memAfter - memBefore;
		IJ.log("Memory usage after [MB]: " + (memAfter/1000000L));
		IJ.log("Memory usage delta [MB]: " + (memDiff / 1000000L));

		// put (local) classification results into  (big) results image
		long startSettingResults = System.currentTimeMillis();

		ExecutorService exe = Executors.newFixedThreadPool( Prefs.getThreads() );

		ArrayList<Future> futures = new ArrayList<>();

		for (int z = 0; z < nz; z++)
		{
			futures.add(
					exe.submit(
						setClassificationResultOld(
								classifiedImage,
								classificationResult,
								region5D,
								z, nx, ny, border)
					)
			);
		}

		trainableDeepSegmentation.utils.Utils.joinThreads(futures); exe.shutdown();

		logger.info("Saved classification results in "
				+ (System.currentTimeMillis() - startSettingResults) + " ms");

		/*
		for (int i = 0; i < numSlices/numChannels; i++)
		{
			for (int c = 0; c < numChannels; c++)
			{
				ImageProcessor ip = classifiedImage.getProcessor();
				byte[] pixels = (byte[]) ip.getPixels();
				int offset = 0; // TODO: compute from region5D
				System.arraycopy( classificationResult[c], i * (nx * ny), pixels, offset, nx * ny);
			}
		}*/

	}


/**
	 * Apply current classifier to a set of feature vectors (given in a feature
	 * stack array). The classification if performed in a multi-threaded way
	 * using as many threads as defined by the user.
	 *
	 * @param featureImages feature stack array
	 * @param numThreads The number of threads to use. Set to zero for auto-detection.
	 * @param probabilityMaps probability flag. Tue: probability maps are calculated, false: binary classification
	 * @return result image containing the probability maps or the binary classification
	 */
	public void applyClassifier(
			final Region5D region5DToClassify,
			final boolean updateFeatureImages,
			final int numThreads,
			final boolean probabilityMaps)
	{

		// TODO: make a local feature images instance!
		//
		logger.info("Applying classifier...");
		logger.info("Memory usage [MB]: " + IJ.currentMemory() / 1000000L + "/" + IJ.maxMemory() / 1000000L);

		long start = System.currentTimeMillis();

		ImagePlus imageToClassify = null;

		if ( region5DToClassify != null )
		{
			if (trainingImage.getImageStack() instanceof VirtualStackOfStacks)
			{
				VirtualStackOfStacks vss = (VirtualStackOfStacks) trainingImage.getStack();
				imageToClassify = vss.getDataCube( region5DToClassify, 0, numThreads );

				logger.info("Loaded " +
						imageToClassify.getWidth() + "x" +
						imageToClassify.getHeight() + "x" +
						imageToClassify.getNSlices()
						+ " pixels in " + (System.currentTimeMillis() - start) + " ms");
			}
			else
			{
				IJ.showMessage("ERROR: no VSS");
				return;
			}
		}
		else
		{
			imageToClassify = trainingImage; // classify whole image
		}


		if ( updateFeatureImages )
		{
			//imageToClassify.show();
			featureImages.setNumFeatureScales(nFeatureScales);
			featureImages.setOriginalImage( imageToClassify );
			featureImages.setEnabledFeatures( enabledFeatures );

			if ( showFeatureImages )
			{
				showFeatureImages = false; // immediately set to false for other threads
				featureImages.updateFeaturesMT( true, featuresToShow );
			}
			else
			{
				featureImages.updateFeaturesMT( false, featuresToShow );
			}

		}


		start = System.currentTimeMillis();

		// border pixels cannot be classified,
		// because the interpolated features
		// cannot not be computed properly
		final int border = getFeatureImagesBorderSize();
		int nx = imageToClassify.getWidth() - 2 * border;
		int ny = imageToClassify.getHeight() - 2 * border;
		int nz = imageToClassify.getNSlices() - 2 * border;

		int nzPerThread = (int) nz / numThreads;
		int nzLastThread = nz - (numThreads - 1) * nzPerThread;

		// create instances information (each instance needs a pointer to this)
		Instances dataInfo = new Instances("segment", getAttributes(featureImages), 1);
		dataInfo.setClassIndex(dataInfo.numAttributes() - 1);

		Future<byte[][]>[] fu = new Future[numThreads];
		Region5D[] region5DThread = new Region5D[numThreads];

		final AtomicInteger slicesClassified = new AtomicInteger();

		int zs = border;

		for( int i = 0; i < numThreads; i++ )
		{
			if (Thread.currentThread().isInterrupted())
				return;

			AbstractClassifier classifierCopy = null;
			try {
				// The Weka random forest classifiers do not need to be duplicated on each thread
				// (that saves much memory)
				if( classifier instanceof FastRandomForest || classifier instanceof RandomForest )
					classifierCopy = classifier;
				else
					classifierCopy = (AbstractClassifier) (AbstractClassifier.makeCopy( classifier ));
			} catch (Exception e) {
				IJ.log("Error: classifier could not be copied to classify in a multi-thread way.");
				e.printStackTrace();
			}

			int nzThread = (i==(numThreads-1)) ? nzLastThread : nzPerThread;

			region5DThread[i] = new Region5D();
			region5DThread[i].size = new Point3D( nx, ny, nzThread );
			region5DThread[i].offset = new Point3D( border, border, zs );
			region5DThread[i].t = 0;
			region5DThread[i].c = 0;
			region5DThread[i].subSampling = new Point3D( 1, 1, 1);

			boolean logging = true;
			fu[i] = exe.submit(
					classifyRegion(
							featureImages,
							region5DThread[i],// todo remove [i] againg if it works
							region5DToClassify,
							dataInfo,
							classifierCopy,
							slicesClassified
					)
			);

			zs += nzThread;
		}

		ArrayList < byte[] > classificationResults = new ArrayList<>();

		// Join threads
		try {
			for(int i = 0; i < numThreads; i++)
			{
				for ( byte[] classifiedSlice : fu[i].get() )
				{
					classificationResults.add( classifiedSlice );
				}
			};
		} catch (InterruptedException e) {
			//e.printStackTrace();
			return;
		} catch (ExecutionException e) {
			e.printStackTrace();
			return;
		} catch ( OutOfMemoryError err ) {
			IJ.log( "ERROR: applyClassifier run out of memory. Please, "
					+ "use a smaller input image or fewer features." );
			err.printStackTrace();
			return;
		}

		final long end = System.currentTimeMillis();

		logger.info("Classified " + nx + "x" + ny + "x" + nz + " pixels in " + (end - start) + " ms");

		// put (local) classification results into  (big) results image
		start = System.currentTimeMillis();

		ArrayList<Future> futures = new ArrayList<>();

		int iSlice = 0;
		for ( byte[] classifiedSlice : classificationResults )
		{
			Region5D region5DThisSlice = new Region5D();
			region5DThisSlice.size = new Point3D( nx, ny, 1 );
			region5DThisSlice.offset = new Point3D(
					(int)region5DToClassify.offset.getX() + border,
					(int)region5DToClassify.offset.getY() + border,
					(int)region5DToClassify.offset.getZ() + border + iSlice );
			region5DThisSlice.t = region5DToClassify.t;
			region5DThisSlice.c = region5DToClassify.c;
			region5DThisSlice.subSampling = new Point3D( 1, 1, 1);

			futures.add(
					exe.submit(
							setClassificationResult(
									classifiedImage,
									region5DThisSlice,
									classifiedSlice
							)
					)
			);

			iSlice++;

		}

		trainableDeepSegmentation.utils.Utils.joinThreads(futures);

		//logger.info("Saved classification results in " + (System.currentTimeMillis() - start) + " ms");

	}



	private Runnable setExampleInstanceValues( Example example )
	{
		if (Thread.currentThread().isInterrupted())
			return null;

		return () -> {

			example.instanceValuesArray = new ArrayList<>();

			Rectangle bounds = example.getBounds();

			// add one to width and height, as, e.g., a horizontal line has zero height.
			int sX = getFeatureVoxelSizeAtMaximumScale() * (2 + (int) Math.ceil((bounds.getWidth() + 1) / getFeatureVoxelSizeAtMaximumScale()));
			int sY = getFeatureVoxelSizeAtMaximumScale() * (2 + (int) Math.ceil((bounds.getHeight() + 1) / getFeatureVoxelSizeAtMaximumScale()));
			int sZ = getFeatureVoxelSizeAtMaximumScale() * 3;

			Point3D center = new Point3D(
					bounds.getX() + bounds.getWidth() / 2,
					bounds.getY() + bounds.getHeight() / 2,
					example.z);

			final Region5D region5D = new Region5D();
			region5D.t = example.t;
			region5D.c = 0;
			region5D.size = new Point3D(sX, sY, sZ);
			region5D.offset = Utils.computeOffsetFromCenterSize(center, region5D.size);
			region5D.subSampling = new Point3D(1, 1, 1);

			// compute feature values for pixels at and around the example
			VirtualStackOfStacks vss = (VirtualStackOfStacks) trainingImage.getStack();

			FeatureImages featureImages = new FeatureImagesMultiResolution();
			featureImages.setNumFeatureScales(nFeatureScales);
			featureImages.setOriginalImage(vss.getDataCube(region5D, 0, 1));
			featureImages.setEnabledFeatures(enabledFeatures);
			featureImages.updateFeaturesMT(false, featuresToShow);
			featureNames = featureImages.getFeatureNames();

			// extract feature values at the z-position of the example
			int z = (int) (example.z - region5D.offset.getZ());
			int nf = featureImages.getNumFeatures();
			double[][][] featureSlice = new double
					[featureImages.getWidth()]
					[featureImages.getHeight()]
					[nf + 1]; // one extra for class label

			featureImages.setInterpolatedFeatureSlice(z, example.t, featureSlice);

			// TODO:
			// - test that above really is the right z plane

			// get and set feature values at the x,y positions of the example
			Point[] points = example.points;

			int randomNum = ThreadLocalRandom.current().nextInt(1, 100 + 1);

			//IJ.log("");
			//IJ.log("NEW EXAMPLE " + randomNum);
			for (Point point : points)
			{

				double[] instanceValues = new double[nf + 1];

				int x = (int) (point.getX() - region5D.offset.getX());
				int y = (int) (point.getY() - region5D.offset.getY());

				/*
				IJ.log("");
				IJ.log(randomNum+" x,y,z global: " + point.getX() + "," + point.getY() + "," + example.z );
				IJ.log(randomNum+" x,y local: " + x + "," + y );

				String[] valueString = new String[4];
				valueString[0] = "";
				valueString[1] = "";
				valueString[2] = "";
				valueString[3] = "";
				*/

				// get the feature values at the x, y, z location
				for (int f = 0; f < nf; f++)
				{
					instanceValues[f] = featureSlice[x][y][f];


					/*if ( f < 7 )
						valueString[0] = valueString[0] + (int)Math.round(instanceValues[f]) + ", ";
					else if ( f < 7+24 )
						valueString[1] = valueString[1] + (int)Math.round(instanceValues[f]) + ", ";
					else if ( f < 7+24+27 )
						valueString[2] = valueString[2] + (int)Math.round(instanceValues[f]) + ", ";
					else
						valueString[3] = valueString[3] + (int)Math.round(instanceValues[f]) + ", ";
						*/
				}

				/*
				for ( String s : valueString )
					IJ.log(randomNum + " x,y,z global: " + point.getX() + "," + point.getY()+ "," + example.z + " values "+s);
					*/
				// Assign class
				instanceValues[instanceValues.length - 1] = (double) example.classNum;

				example.instanceValuesArray.add(instanceValues);

				example.featureNames = new ArrayList<>();
				for ( String featureName : featureImages.getFeatureNames() )
					example.featureNames.add( featureName );

			}


		};

	}



	/**
	 * Save specific slice feature stack
	 *
	 * @param slice slice number
	 * @param dir directory to save the stack(s)
	 * @param fileWithExt file name with extension for the file(s)
	 */
	public void saveFeatureStack(int slice, String dir, String fileWithExt)
	{

		if( featureImages.isEmpty() ||
				featureImages.getReferenceSliceIndex() == -1)
		{
			IJ.showStatus("Creating feature stack...");
			IJ.log("Creating feature stack...");

			featureImages.updateFeaturesMT( showFeatureImages, featuresToShow );

		}

		if(null == dir || null == fileWithExt)
			return;


		final String fileName = dir + fileWithExt.substring(0, fileWithExt.length()-4)
									+ String.format("%04d", slice) + ".tif";
		/*
		if(!featureImages.saveStackAsTiff(fileName))
		{
			IJ.error("Error", "Feature stack could not be saved");
			return;
		}

		IJ.log("Saved feature stack for slice " + (slice) + " as " + fileName);
		*/
	}


	/**
	 * Save training data into a file (.arff)
	 * @param pathname complete path name
	 * @return false if error
	 */
	public boolean saveData(final String pathname)
	{
		// TODO: implement this!
		boolean examplesEmpty = true;
		for(int i = 0; i < numOfClasses; i ++)
		{
			for(int n=0; n<trainingImage.getImageStackSize(); n++)
				if( getNumExamples(i) > 0 )
				{
					examplesEmpty = false;
					break;
				}
		}
		if (examplesEmpty && loadedTrainingData == null){
			IJ.log("There is no data to save");
			return false;
		}

		if( updateFeatures )
		{
			IJ.log("Creating feature stack...");

			if( ! featureImages.updateFeaturesMT( showFeatureImages, featuresToShow) )
				return false;

			//filterFeatureStackByList();
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}

		Instances data = null;

		if(!examplesEmpty)
		{
			data = createTrainingInstances();
			data.setClassIndex(data.numAttributes() - 1);
		}
		if (null != loadedTrainingData && null != data){
			IJ.log("Merging data...");
			for (int i=0; i < loadedTrainingData.numInstances(); i++){
				// IJ.log("" + i)
				data.add(loadedTrainingData.instance(i));
			}
			IJ.log("Finished: total number of instances = " + data.numInstances());
		}
		else if (null == data)
			data = loadedTrainingData;


		IJ.log("Writing training data: " + data.numInstances() + " instances...");

		//IJ.log("Data: " + data.numAttributes() +" attributes, " + data.numClasses() + " classes");

		writeDataToARFF(data, pathname);
		IJ.log("Saved training data: " + pathname);

		return true;
	}



	/**
	 * Load a new image to segment (no GUI)
	 *
	 * @param newImage new image to segment
	 * @return false if error
	 */
	public boolean loadNewImage( ImagePlus newImage )
	{
		// Accumulate current data in "loadedTrainingData"
		IJ.log("Storing previous image instances...");

		if( updateFeatures )
		{
			IJ.log("Creating feature stack...");
			if ( !featureImages.updateFeaturesMT(showFeatureImages, featuresToShow) )
				return false;
			updateFeatures = false;
			IJ.log("Feature stack is now updated.");
		}

		// Create instances
		Instances data = createTrainingInstances();
		if (null != loadedTrainingData && null != data)
		{
			data.setClassIndex(data.numAttributes() - 1);
			IJ.log("Merging data...");
			for (int i=0; i < loadedTrainingData.numInstances(); i++){
				// IJ.log("" + i)
				data.add(loadedTrainingData.instance(i));
			}
			IJ.log("Finished");
		}
		else if (null == data)
			data = loadedTrainingData;

		// Store merged data as loaded data
		loadedTrainingData = data;

		if(null != loadedTrainingData)
		{
			Attribute classAttribute = loadedTrainingData.classAttribute();
			Enumeration<Object> classValues  = classAttribute.enumerateValues();

			// Update list of names of loaded classes
			loadedClassNames = new ArrayList<String>();
			while(classValues.hasMoreElements())
			{
				final String className = ( (String) classValues.nextElement() ).trim();
				loadedClassNames.add(className);
			}
			IJ.log("Number of accumulated examples: " + loadedTrainingData.numInstances());
		}
		else
			IJ.log("Number of accumulated examples: 0");

		// Updating image
		IJ.log("Updating image...");

		// Set new image as training image
		trainingImage = new ImagePlus("Advanced Weka Segmentation", newImage.getImageStack());

		// Initialize feature stack array (no features yet)
		featureImages = new FeatureImagesMultiResolution( trainingImage );

		// Remove traces from the lists and ROI overlays and initialize each feature stack
		IJ.log("Removing previous markings...");
		examples = new ArrayList<>();

		/*
		examples = new Vector[trainingImage.getImageStackSize()];
		for(int i=0; i< trainingImage.getImageStackSize(); i++)
		{
			examples[i] = new Vector<ArrayList<Roi>>(MAX_NUM_CLASSES);

			for(int j=0; j<MAX_NUM_CLASSES; j++)
				examples[i].add(new ArrayList<Roi>());

		}*/

		updateFeatures = true;

		// Remove current classification result image
		classifiedImage = null;

		IJ.log("New image: " + newImage.getTitle() + " ("+trainingImage.getImageStackSize() + " slice(s))");

		IJ.log("Done");

		return true;
	}


	/**
	 * Classify instances concurrently
	 *
	 * @param featureImages feature stack array with the feature vectors
	 * @param dataInfo empty set of instances containing the data structure (attributes and classes)
	 * @param first index of the first instance to classify (considering the feature stack array as a 1D array)
	 * @param numInstances number of instances to classify in this thread
	 * @param classifier current classifier
	 * @param counter auxiliary counter to be able to update the progress bar
	 * @param probabilityMaps if true return a probability map for each class instead of a classified image
	 * @return classification result
	 */
	private static Callable<byte[][]> classifyInstances(
			final FeatureImages featureImages,
			final int frame,
			final Instances dataInfo,
			final int first,
			final int numInstances,
			final int border,
			final AbstractClassifier classifier,
			final AtomicInteger counter,
			final boolean probabilityMaps)
	{
		if (Thread.currentThread().isInterrupted())
			return null;

		return new Callable<byte[][]>(){

			@Override
			public byte[][] call(){

				final byte[][] classificationResult;

				final int nx = featureImages.getWidth();
				final int ny = featureImages.getHeight();
				final int nz = featureImages.getDepth();
				final int sliceSize = nx * ny;
				final int numClasses = dataInfo.numClasses();

				if (probabilityMaps)
					classificationResult = new byte[numClasses][numInstances];
				else
					classificationResult = new byte[1][numInstances];

				// auxiliary array to be filled for each instance
				final double[] values = new double[ featureImages.getNumFeatures() + 1 ];
				// create empty reusable instance
				final ReusableDenseInstance ins =
						new ReusableDenseInstance( 1.0, values );
				ins.setDataset(dataInfo);

				long tStart, durationFeatureFetching = 0 , durationRandomForest = 0;

				int zPrevious = -1;

				// create reusable feature slice
				double[][][] featureSlice = new double
						[featureImages.getWidth()]
						[featureImages.getHeight()]
						[featureImages.getNumFeatures() + 1]; // one extra for class label

				for ( int i = 0; i < numInstances; i++ )
				{

					try{

						tStart = System.nanoTime();

						final int absolutePos = first + i;
						final int z = absolutePos / sliceSize; // one-based
						final int localPos = absolutePos - z * sliceSize;
						final int x = localPos % nx;
						final int y = localPos / nx;

						if ( (x > border) && (y > border) && ( z > border)
								&& ( x < (nx-border) ) && ( y < (ny-border) ) && ( z  < (nz-border) ))
						{
							if (z != zPrevious)
							{
								if (zPrevious >= 0)
								{
									counter.getAndAdd(1);
									String[] logs = IJ.getLog().split("\n");
									if (logs[logs.length - 1].contains("Classifying slice"))
									{
										IJ.log("\\Update:Classifying slice: " + counter +
														"/"+nz+"; Feature fetching [ms]: " +
														Math.round((double) durationFeatureFetching / 1000000L) +
														"; Classification [ms]: " +
														Math.round((double) durationRandomForest / 1000000L)
										);

										// restart the timing
										tStart = System.nanoTime();
										durationFeatureFetching = 0;
										durationRandomForest = 0;
									}
									else
									{
										IJ.log("Classifying slice: " + counter);
									}
								}


								if (Thread.currentThread().isInterrupted())
									return null;

								zPrevious = z;

								// set reusable featureSlice
								featureImages.setInterpolatedFeatureSlice(z, frame - 1, featureSlice);

							}

							// set reusable instance
							ins.setValues(1.0, featureSlice[x][y]);

							durationFeatureFetching += (System.nanoTime() - tStart);

							// do random forest classification

							tStart = System.nanoTime();

							if ( probabilityMaps )
							{
								double[] prob = classifier.distributionForInstance(ins);
								for (int k = 0; k < numClasses; k++)
									classificationResult[k][i] = (byte) (100 * prob[k]);
							}
							else
							{
								classificationResult[0][i] = (byte) classifier.classifyInstance(ins);
							}

							durationRandomForest += ( System.nanoTime() - tStart );

						}


					}
					catch(Exception e)
					{

						IJ.showMessage("Could not apply Classifier!");
						e.printStackTrace();
						return null;
					}
				}

				//IJ.log( "Duration feature fetching [s]: " + Math.round( (double)durationFeatureFetching/1000000000L ) );
				//IJ.log( "Duration random forrest [s]: " + Math.round( (double) durationRandomForest/1000000000L ) );


				return classificationResult;
			}
		};
	}

	/**
	 * Get confusion matrix (2 classes)
	 * @param proposal probability image
	 * @param expectedLabels original labels
	 * @param threshold binary threshold to be applied to proposal
	 * @return confusion matrix
	 */
	public static int[][] getConfusionMatrix(
			ImagePlus proposal,
			ImagePlus expectedLabels,
			double threshold)
	{
		int[][] confusionMatrix = new int[2][2];

		final int depth = proposal.getStackSize();

		ExecutorService exe = Executors.newFixedThreadPool( numThre );
		ArrayList< Future <int[][]>  > fu = new ArrayList<Future <int[][]>>();

		// Compare labels
		for(int z=1; z <= depth; z++)
		{
			fu.add( exe.submit( confusionMatrixBinarySlice(proposal.getImageStack().getProcessor( z ), expectedLabels.getImageStack().getProcessor( z ), threshold)) );
		}

		for(int z=0; z < depth; z++)
		{
			try {
				int[][] temp = fu.get( z ).get();
				for(int i=0 ; i<2; i++)
					for(int j=0 ; j<2; j++)
						confusionMatrix[i][j] += temp[i][j];

			} catch (Exception e) {
				e.printStackTrace();
				return null;
			}
			finally{
				exe.shutdown();
			}
		}


		return confusionMatrix;
	}


		/**
    	 * Apply classifier to test data. As it is implemented right now,
    	 * it will use one thread per input image and slice.
    	 */
    	public void applyClassifierToTestData()
    	{
    		// array of files to process
    		File[] imageFiles;
    		String storeDir = "";

    		// create a file chooser for the image files
    		String dir = OpenDialog.getLastDirectory();
    		if (null == dir)
    			dir = OpenDialog.getDefaultDirectory();
    		if( Prefs.useFileChooser )
    		{
    			JFileChooser fileChooser = new JFileChooser( dir );
    			fileChooser.setFileSelectionMode(JFileChooser.FILES_ONLY);
    			fileChooser.setMultiSelectionEnabled(true);
    			fileChooser.setDialogTitle( "Select file(s) to classify" );

    			// get selected files or abort if no file has been selected
    			int returnVal = fileChooser.showOpenDialog(null);
    			if(returnVal == JFileChooser.APPROVE_OPTION) {
    				imageFiles = fileChooser.getSelectedFiles();
    				OpenDialog.setLastDirectory( imageFiles[ 0 ].getParent() );
    			} else {
    				return;
    			}
    		}
    		else // use FileDialog
    		{
    			final Frame parent = IJ.getInstance();
    			final FileDialog fd = new FileDialog( parent,
    					"Select file(s) to classify", FileDialog.LOAD );
    			fd.setDirectory( dir );
    			fd.setMultipleMode( true );
    			// files only
    			fd.setFilenameFilter( new FilenameFilter(){
    				public boolean accept( File dir, String name )
    				{
    					final File f = new File( dir + File.separator + name );
    					if( f.exists() && !f.isDirectory() )
    						return true;
    					else
    						return false;
    				}
    			});
    			// get selected files or abort if no file has been selected
    			fd.setVisible( true );
    			imageFiles = fd.getFiles();
    			if( null == imageFiles || imageFiles.length == 0 )
    				return;
    			else
    				OpenDialog.setLastDirectory( imageFiles[ 0 ].getParent() );
    		}

    		boolean showResults = true;
    		boolean storeResults = false;

    		if (imageFiles.length >= 3) {

    			int decision = JOptionPane.showConfirmDialog(null, "You decided to process three or more image " +
    					"files. Do you want the results to be stored on the disk instead of opening them in Fiji?",
    					"Save results?", JOptionPane.YES_NO_OPTION);

    			if (decision == JOptionPane.YES_OPTION) {
    				final DirectoryChooser dc = new DirectoryChooser(
    						"Select folder to store results");
    				DirectoryChooser.setDefaultDirectory( dir );
    				storeDir = dc.getDirectory();
    				if( null == storeDir )
    					return;

    				showResults  = false;
    				storeResults = true;
    			}
    		}

    		final boolean probabilityMaps;

    		int decision = JOptionPane.showConfirmDialog(null, "Create probability maps instead of segmentation?", "Probability maps?", JOptionPane.YES_NO_OPTION);
    		if (decision == JOptionPane.YES_OPTION)
    			probabilityMaps = true;
    		else
    			probabilityMaps = false;

    		final int numProcessors     = Prefs.getThreads();
    		final int numThreads        = Math.min(imageFiles.length, numProcessors);
    		final int numFurtherThreads = (int)Math.ceil((double)(numProcessors - numThreads)/imageFiles.length) + 1;

    		IJ.log("Processing " + imageFiles.length + " image file(s) in " + numThreads + " thread(s)....");

    		win.setButtonsEnabled(false);

    		Thread[] threads = new Thread[numThreads];

    		class ImageProcessingThread extends Thread {

    			private final int     numThread;
    			private final int     numThreads;
    			private final File[]  imageFiles;
    			private final boolean storeResults;
    			private final boolean showResults;
    			private final String  storeDir;

    			public ImageProcessingThread(int numThread, int numThreads,
    			                             File[] imageFiles,
    			                             boolean storeResults, boolean showResults,
    			                             String storeDir) {
    				this.numThread     = numThread;
    				this.numThreads    = numThreads;
    				this.imageFiles    = imageFiles;
    				this.storeResults  = storeResults;
    				this.showResults   = showResults;
    				this.storeDir      = storeDir;
    			}

    			public void run() {

    				// TODO: implement
    				/*
    				for (int i = numThread; i < imageFiles.length; i += numThreads) {
    					File file = imageFiles[i];

    					ImagePlus testImage = IJ.openImage(file.getPath());

    					if( null == testImage )
    					{
    						IJ.log( "Error: " + file.getPath() + " is not a valid image file.");
    						IJ.error("Trainable Weka Segmentation I/O error", "Error: " + file.getPath() + " is not a valid image file.");
    						return;
    					}

    					IJ.log("Processing image " + file.getName() + " in thread " + numThread);

    					ImagePlus segmentation =
    							wekaSegmentation.applyClassifier( testImage,
    									numFurtherThreads, probabilityMaps );
    					if( null == segmentation )
    					{
    						IJ.log( "Error: " + file.getName() + "could not be "
    								+ "classified!" );
    						return;
    					}
    					if ( !probabilityMaps )
    					{
    						// convert slices to 8-bit and apply overlay LUT
    						convertTo8bitNoScaling( segmentation );
    						segmentation.getProcessor().setColorModel( overlayLUT );
    						segmentation.getImageStack().setColorModel( overlayLUT );
    						segmentation.updateAndDraw();
    					}

    					if ( showResults )
    					{
    						segmentation.show();
    						testImage.show();
    					}
    					else if ( storeResults ) {
    						String filename = storeDir + File.separator + file.getName();
    						IJ.log("Saving results to " + filename);
    						IJ.save(segmentation, filename);
    						segmentation.close();
    						testImage.close();
    						// force garbage collection
    						segmentation = null;
    						testImage = null;
    						System.gc();
    					}
    				}*/
    			}
    		}

    		// start threads
    		for (int i = 0; i < numThreads; i++) {

    			threads[i] = new ImageProcessingThread(i, numThreads, imageFiles, storeResults, showResults, storeDir);
    			// Record
    			String[] arg = new String[] {
    				imageFiles[i].getParent(),
    				imageFiles[i].getName(),
    				"showResults=" + showResults,
    				"storeResults=" + storeResults,
    				"probabilityMaps="+ probabilityMaps,
    				storeDir	};
    			record(APPLY_CLASSIFIER, arg);
    			threads[i].start();
    		}

    		// join all threads
    		for (Thread thread : threads) {
    			try {
    				thread.join();
    			} catch (InterruptedException e) {}
    		}

    		win.updateButtonsEnabling();

    	}


    		/**
        	 * Assign an arbitrary filter stack array
        	 * @param fsa new filter stack array
        	 */
        	public void setFeatureImages(FeatureImages fsa)
        	{
        		/*
        		this.featureImages = fsa;
        		// set feature stacks to be updated during train and test to false
        		// (since the feautures are set externally and expected to be up to date)
        		featureStackToUpdateTrain = new boolean[featureImages.getNumFeatures()];
        		featureStackToUpdateTest = new boolean[featureImages.getNumFeatures()];
        		Arrays.fill(featureStackToUpdateTest, false);
        		// set flag to not update features
        		updateFeatures = false;
        		*/
        	}


	/**
	 * Create instances of a feature stack (to be submitted to an Executor Service)
	 *
	 * @param classNames names of the classes of data
	 * @param featureStack feature stack to create the instances from
	 * @return set of instances
	 */
	public Callable<Instances> createInstances(
			final ArrayList<String> classNames,
			final FeatureStack featureStack)
	{
		if (Thread.currentThread().isInterrupted())
			return null;

		return new Callable<Instances>(){
			@Override
			public Instances call()
			{
				return featureStack.createInstances(classNames);
			}
		};
	}


	    /**
         * Create instances for whole stack
         *
         * @param classes list of classes names
         *
         * @return whole stack set of instances
         */
        public Instances createInstances(ArrayList<String> classes)
        {
            // TODO: what is this doing? feels like it is only for 2-D?

            IJ.showMessage("NOT IMPLEMENTED: createInstances(ArrayList<String> classes)");

            if (Thread.currentThread().isInterrupted())
                return null;

            /*
            if( oldColorFormat )
                IJ.log( "Using old color format...");
            */

            // get feature names
            ArrayList<Attribute> attributes = getFeatureNamesAsAttributes();

            attributes.add(new Attribute("class", classes));

            Instances data = new Instances("segment", attributes, width*height);

            for (int y=0; y<width; y++)
            {
                if (Thread.currentThread().isInterrupted())
                    return null;
                IJ.showProgress(y, width);
                for (int x=0; x<height; x++)
                {
                    data.add(createInstance(x, y, 0, 0, 0));
                }
            }
            // Set the index of the class attribute
            data.setClassIndex( attributes.size() - 1 );
            IJ.showProgress(1.0);
            return data;
        }



	/**
	 * Load training data
	 *
	 * @param pathname complete path name of the training data file (.arff)
	 * @return false if error
	 */
	public boolean loadTrainingData(String pathname)
	{
		IJ.log("Loading data from " + pathname + "...");
		loadedTrainingData = readDataFromARFF(pathname);
		if( null == loadedTrainingData )
		{
			IJ.log( "Unable to load training data from " + pathname );
			return false;
		}

		// Check the features that were used in the loaded data
		Enumeration<Attribute> attributes = loadedTrainingData.enumerateAttributes();
		final String[] availableFeatures = new FeatureImagesMultiResolution().availableFeatures;

		final int numFeatures = availableFeatures.length;
		boolean[] usedFeatures = new boolean[numFeatures];
		while(attributes.hasMoreElements())
		{
			final Attribute a = attributes.nextElement();
			for(int i = 0 ; i < numFeatures; i++)
				if(a.name().startsWith(FeatureStack.availableFeatures[i]))
					usedFeatures[i] = true;
		}

		// Check if classes match
		Attribute classAttribute = loadedTrainingData.classAttribute();
		Enumeration<Object> classValues  = classAttribute.enumerateValues();

		// Update list of names of loaded classes
		loadedClassNames = new ArrayList<String>();

		int j = 0;
		while(classValues.hasMoreElements())
		{
			final String className = ((String)classValues.nextElement()).trim();
			loadedClassNames.add(className);

			IJ.log("Read class name: " + className);
			if( !className.equals(this.getClassNames()[j]))
			{
				String s = getClassNames()[0];
				for(int i = 1; i < numOfClasses; i++)
					s = s.concat(", " + getClassNames()[i]);
				IJ.error("ERROR: Loaded classes and current classes do not match!\nExpected: " + s);
				loadedTrainingData = null;
				return false;
			}
			j++;
		}

		if(j != numOfClasses)
		{
			IJ.error("ERROR: Loaded number of classes and current number do not match!");
			loadedTrainingData = null;
			return false;
		}

		boolean featuresChanged = false;
		final boolean[] oldEnableFeatures = enabledFeatures;
		// Read checked features and check if any of them chasetButtonsEnablednged
		for(int i = 0; i < numFeatures; i++)
		{
			if (usedFeatures[i] != oldEnableFeatures[i])
				featuresChanged = true;
		}
		// Update feature stack if necessary
		if( featuresChanged )
		{
			//this.setButtonsEnabled(false);
			this.setEnabledFeatures( usedFeatures );
			// Force features to be updated
			updateFeatures = true;

		}

		if (!adjustSegmentationStateToData(loadedTrainingData))
			loadedTrainingData = null;
		else
			IJ.log("Loaded data: " + loadedTrainingData.numInstances() + " instances (" + loadedTrainingData.numAttributes() + " attributes)");

		return true;
	}

	/**
	 * Returns a the loaded training data or null, if no training data was
	 * loaded.
	 */
	public Instances getLoadedTrainingData() {
		return loadedTrainingData;
	}

	/**
	 * Returns a the trace training data or null, if no examples have been
	 * given.
	 */
	public Instances getTraceTrainingData() {
		return traceTrainingData;
	}

	/**


		/**
    	 * Set features to use during training
    	 *
    	 * @param featureNames list of feature names to use
    	 * @return false if error
    	 */
    	public boolean setFeatures(ArrayList<String> featureNames)
    	{
    		if (null == featureNames)
    			return false;

    		this.featureNames = featureNames;

    		final int numFeatures = FeatureStack.availableFeatures.length;
    		boolean[] usedFeatures = new boolean[numFeatures];
    		for(final String name : featureNames)
    		{
    			for(int i = 0 ; i < numFeatures; i++)
    				if(name.startsWith(FeatureStack.availableFeatures[i]))
    					usedFeatures[i] = true;
    		}

    		return true;
    	}


	/** Disables features which rely on missing third party libraries. */
	public static void disableMissingFeatures(final GenericDialog gd)
	{
		if (!isImageScienceAvailable()) {
			IJ.log("Warning: ImageScience library unavailable. " +
				"Some training features will be disabled.");
			@SuppressWarnings("unchecked")
			final Vector<Checkbox> v = gd.getCheckboxes();
			for (int i = 0; i < v.size(); i++) {
				if (FeatureStack.IMAGESCIENCE_FEATURES[i]) {
					v.get(i).setState(false);
					v.get(i).setEnabled(false);
				}
			}
		}
	}


		/**
    	 * Plot the current result
    	 */
    	void plotResult()
    	{
    		IJ.showStatus("Evaluating current data...");
    		IJ.log("Evaluating current data...");
    		win.setButtonsEnabled(false);
    		final Instances data;
    		if (wekaSegmentation.getTraceTrainingData() != null)
    			data = wekaSegmentation.getTraceTrainingData();
    		else
    			data = wekaSegmentation.getLoadedTrainingData();

    		if(null == data)
    		{
    			IJ.error("Error in plot result",
    					"No data available yet to plot results: you need to trace\n"
    							+ "some training samples or load data from file.");
    			win.updateButtonsEnabling();
    			return;
    		}

    		displayGraphs(data, wekaSegmentation.getClassifier());
    		win.updateButtonsEnabling();
    		IJ.showStatus("Done.");
    		IJ.log("Done");
    	}

    	/**
    	 * Display the threshold curve window (for precision/recall, ROC, etc.).
    	 *
    	 * @param data input instances
    	 * @param classifier classifier to evaluate
    	 */
    	public static void displayGraphs(Instances data, AbstractClassifier classifier)
    	{
    		ThresholdCurve tc = new ThresholdCurve();

    		ArrayList<Prediction> predictions = null;
    		try {
    			final EvaluationUtils eu = new EvaluationUtils();
    			predictions = eu.getTestPredictions(classifier, data);
    		} catch (Exception e) {
    			IJ.log("Error while evaluating data!");
    			e.printStackTrace();
    			return;
    		}

    		Instances result = tc.getCurve(predictions);
    		ThresholdVisualizePanel vmc = new ThresholdVisualizePanel();
    		vmc.setName(result.relationName() + " (display only)");
    		PlotData2D tempd = new PlotData2D(result);
    		tempd.setPlotName(result.relationName());
    		tempd.addInstanceNumberAttribute();
    		try {
    			vmc.addPlot(tempd);
    		} catch (Exception e) {
    			IJ.log("Error while adding plot to visualization panel!");
    			e.printStackTrace();
    			return;
    		}
    		String plotName = vmc.getName();
    		JFrame jf = new JFrame("Weka Classifier Visualize: "+plotName);
    		jf.setSize(500, 400);
    		jf.getContentPane().setLayout(new BorderLayout());
    		jf.getContentPane().add(vmc, BorderLayout.CENTER);
    		jf.setVisible(true);
    	}


	/**
	 * Plot the current result (threshold curves)
	 */
	public static void plotResultGraphs()
	{
		final ImageWindow iw = WindowManager.getCurrentImage().getWindow();
		if( iw instanceof CustomWindow )
		{
			final CustomWindow win = (CustomWindow) iw;
			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();

			IJ.showStatus("Evaluating current data...");
			IJ.log("Evaluating current data...");
			win.setButtonsEnabled(false);
			final Instances data;
			if (wekaSegmentation.getTraceTrainingData() != null)
				data = wekaSegmentation.getTraceTrainingData();
			else
				data = wekaSegmentation.getLoadedTrainingData();

			if(null == data)
			{
				IJ.error("Error in plot result", "No data available yet to display results");
				return;
			}

			displayGraphs(data, wekaSegmentation.getClassifier());
			win.updateButtonsEnabling();
			IJ.showStatus("Done.");
			IJ.log("Done");
		}
	}


	/**
	 * Apply current classifier to specific image (2D or stack)
	 *
	 * @param dir input image directory path
	 * @param fileName input image name
	 * @param showResultsFlag string containing the boolean flag to display results
	 * @param storeResultsFlag string containing the boolean flag to store result in a directory
	 * @param probabilityMapsFlag string containing the boolean flag to calculate probabilities instead of a binary result
	 * @param storeDir directory to store the results
	 */
	public static void applyClassifier(
			String dir,
			String fileName,
			String showResultsFlag,
			String storeResultsFlag,
			String probabilityMapsFlag,
			String storeDir)
	{
		/*
		final ImageWindow iw = WindowManager.getCurrentImage().getWindow();
		if( iw instanceof CustomWindow )
		{
			final CustomWindow win = (CustomWindow) iw;
			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();
			ImagePlus testImage =
					IJ.openImage( dir + File.separator + fileName );

			if(null == testImage)
			{
				IJ.log("Error: " + dir + File.separator	+ fileName
						+ " could not be opened");
				return;
			}

			boolean probabilityMaps = probabilityMapsFlag.contains("true");
			boolean storeResults = storeResultsFlag.contains("true");
			boolean showResults = showResultsFlag.contains("true");

			IJ.log( "Processing image " + dir + File.separator + fileName );

			ImagePlus segmentation = wekaSegmentation.applyClassifier(testImage, 0, probabilityMaps);

			if( !probabilityMaps )
			{
				// apply LUT to result image
				convertTo8bitNoScaling( segmentation );
				segmentation.getProcessor().setColorModel( win.getOverlayLUT() );
				segmentation.getImageStack().setColorModel( win.getOverlayLUT() );
				segmentation.updateAndDraw();
			}

			if (showResults)
			{
				segmentation.show();
				testImage.show();
			}

			if (storeResults)
			{
				String filename = storeDir + File.separator + fileName;
				IJ.log("Saving results to " + filename);
				IJ.save(segmentation, filename);
				segmentation.close();
				testImage.close();
			}
		}*/
	}


		/**
    	 * Load training data from file
    	 *
    	 * @param arffFilePathName complete path name of the ARFF file
    	 */
    	public static void loadData(String arffFilePathName )
    	{
    		final ImageWindow iw = WindowManager.getCurrentImage().getWindow();
    		if( iw instanceof CustomWindow )
    		{
    			final CustomWindow win = (CustomWindow) iw;
    			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();

    			win.setButtonsEnabled(false);
    			IJ.log("Loading data from " + arffFilePathName + "...");
    			wekaSegmentation.loadTrainingData( arffFilePathName );
    			win.updateButtonsEnabling();
    		}
    	}

    	/**
    	 * Save training data into an ARFF file
    	 *
    	 * @param arffFilePathName complete path name of the ARFF file
    	 */
    	public static void saveData(String arffFilePathName)
    	{
    		/*
    		final ImageWindow iw = WindowManager.getCurrentImage().getWindow();
    		if( iw instanceof CustomWindow )
    		{
    			final CustomWindow win = (CustomWindow) iw;
    			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();

    			if( !wekaSegmentation.saveData( arffFilePathName ))
    				IJ.showMessage("There is no data to save");
    		}*/
    	}


    		private static Runnable setClassificationResultOld(
        			ImagePlus classifiedImage,
        			byte[][] classificationResult,
        			Region5D region5D,
        			int z, int nx, int ny, int border
        	)
        	{

        		if (Thread.currentThread().isInterrupted())
        			return null;

        		return () -> {

        			VirtualStackOfStacks classifiedImageStack = (VirtualStackOfStacks) classifiedImage.getStack();

        			int sliceDest = (int) region5D.offset.getZ() + z + 1;
        			byte[] pixels = (byte[]) classifiedImageStack.getProcessor(sliceDest).getPixels();

        			for (int y = border; y < ny - border; y++)
        			{
        				int offsetDest =
        						(int) region5D.offset.getY() * classifiedImage.getWidth()
        						+ y * classifiedImage.getWidth()
        						+ (int) region5D.offset.getX() + border;

        				int offsetSource = z * (nx * ny) + y * nx + border;

        				System.arraycopy(classificationResult[0], offsetSource, pixels, offsetDest, nx - 2 * border);
        			}

        			//classifiedImageStack.setAndSavePixels(pixels, sliceDest);

        		};

        	}



    /**
     * Create instance (feature vector) of a specific coordinate
     *
     * @param x x- axis coordinate
     * @param y y- axis coordinate
     * @param classValue class value to be assigned
     * @return corresponding instance
     */
    public DenseInstance createInstance(
            int x,
            int y,
            int z,
            int t,
            int classValue )
    {

        final double[] values = new double[ getNumFeatures() + 1 ];

        // get the feature values at the x, y, z location
        for ( int i = 0; i < getNumFeatures(); i++ )
        {
            values[ i ] = getFeatureValue(x, y, z, t, i);
        }

        // Assign class
        values[values.length-1] = (double) classValue;

        return new DenseInstance(1.0, values);
    }


    public double[] getInstanceValues(
            int x,
            int y,
            int z,
            int t,
            int classValue)
    {
        final double[] values = new double[ getNumFeatures() + 1 ];

        // get the feature values at the x, y, z location
        for ( int i = 0; i < getNumFeatures(); i++ )
        {
            values[ i ] = getFeatureValue(x, y, z, t, i);
        }

        // Assign class
        values[values.length-1] = (double) classValue;

        return values;
    }

    /**
     * Set values to an instance (feature vector) of a specific coordinate.
     * The input instance needs to have a data set assigned.
     *
     * @param x x- axis coordinate
     * @param y y- axis coordinate
     * @param sliceNum z- axis coordinate
     * @param classValue class value to be assigned
     * @param ins instance to be filled
     * @param auxArray auxiliary array to store feature values
     */
    public void setInstance(
            int x,
            int y,
            int z,
            int t,
            int classValue,
            final ReusableDenseInstance ins,
            final double[] auxArray )
    {
        // get the feature values at the x, y, z location
        for ( int i = 0; i < getNumFeatures(); i++ )
        {
            auxArray[ i ] = getFeatureValue(x, y, z, t, i);
        }

        // Assign class
        auxArray[ auxArray.length - 1 ] = (double) classValue;

        // Set attribute values to input instance
        ins.setValues(1.0, auxArray);
        return;
    }

        /**
         * Get value of the feature
         *
         * @param x x- axis coordinate
         * @param y y- axis coordinate
         * @param z z- axis coordinate (zero-based)
         * @param t t- axis coordinate (zero-based)
         * @param i Feature number
         * @return value of the feature
         */
        public double getFeatureValue( int x, int y, int z, int t, int iFeature )
        {
            ImagePlus imp = multiResolutionFeatureImageArray[ t ][ iFeature ];

            Calibration calibration = imp.getCalibration();

            int zSlice =  (int) ( z / calibration.pixelDepth ) + 1;

            if ( zSlice == imp.getNSlices()+1 )
            {
                // this can happen due to the binning
                zSlice = imp.getNSlices();
            }

            ImageProcessor ip = imp.getStack().getProcessor(zSlice);

            double value = ip.getPixelValue( (int) (x / calibration.pixelWidth) , (int) (y / calibration.pixelHeight) );

            return value;
        }

            public boolean saveStackAsTiff( String filePath )
            {
                IJ.showMessage("not implemented");
                return true;
            }


            	/**
            	 * Adjust current segmentation state (attributes and classes) to
            	 * loaded data
            	 * @param data loaded instances
            	 * @return false if error
            	 */
            	public boolean adjustSegmentationStateToData(Instances data)
            	{
            		// Check the features that were used in the loaded data
            		boolean featuresChanged = false;
            		Enumeration<Attribute> attributes = data.enumerateAttributes();
            		final String[] availableFeatures =
            					new FeatureImagesMultiResolution().availableFeatures;
            		final int numFeatures = availableFeatures.length;
            		boolean[] usedFeatures = new boolean[numFeatures];

            		// Initialize list of names for the features to use
            		this.featureNames = new ArrayList<String>();

            		float minSigma = Float.MAX_VALUE;
            		float maxSigma = Float.MIN_VALUE;

            		while(attributes.hasMoreElements())
            		{
            			final Attribute a = attributes.nextElement();
            			this.featureNames.add(a.name());
            			for(int i = 0 ; i < numFeatures; i++)
            			{
            				if(a.name().startsWith( availableFeatures[i] ))
            				{
            					usedFeatures[i] = true;
            					String[] tokens;
            					float sigma;

            					tokens = a.name().split("_");
            					for(int j=0; j<tokens.length; j++)
            						if(tokens[j].indexOf(".") != -1)
            						{
            							sigma = Float.parseFloat(tokens[j]);
            							if(sigma < minSigma)
            								minSigma = sigma;
            							if(sigma > maxSigma)
            								maxSigma = sigma;
            						}
            				}
            			}
            		}

            		// Check if classes match
            		Attribute classAttribute = data.classAttribute();
            		Enumeration<Object> classValues  = classAttribute.enumerateValues();

            		// Update list of names of loaded classes
            		loadedClassNames = new ArrayList<String>();

            		int j = 0;
            		setNumOfClasses(0);

            		while(classValues.hasMoreElements())
            		{
            			final String className = ( (String) classValues.nextElement() ).trim();
            			loadedClassNames.add(className);
            		}

            		for(String className : loadedClassNames)
            		{
            			IJ.log("Read class name: " + className);
            			setClassLabel(j, className);
            			addClass();
            			j++;
            		}

            		if( null != featureImages)
            		{
            			final boolean[] oldEnableFeatures =
            						featureImages.getEnabledFeatures();
            			// Read checked features and check if any of them changed
            			for(int i = 0; i < numFeatures; i++)
            			{
            				if (usedFeatures[i] != oldEnableFeatures[i])
            					featuresChanged = true;
            			}
            		}
            		else
            			featuresChanged = true;

            		// Update feature stack if necessary
            		if(featuresChanged)
            		{
            			//this.setButtonsEnabled(false);
            			this.setEnabledFeatures( usedFeatures );
            			// Force features to be updated
            			updateFeatures = true;
            		}

            		return true;
            	}

    /**
     * Set current training header, and attempt to adjust segmentation state to it.
     * @param newHeader the header to set
     * @return true if adjustment was successful
     */
    public boolean setTrainHeader(final Instances newHeader)
    {
     	/*
        if (adjustSegmentationStateToData(newHeader))
        {
            this.trainHeader = newHeader;
            return true;
        }
        else
        {
            return false;
        }
        */
    }


    	/**
    	 * Train the current classifier
    	 */
    	public static void trainClassifier()
    	{
    		final ImageWindow iw = WindowManager.getCurrentImage().getWindow();
    		if( iw instanceof CustomWindow )
    		{
    			final CustomWindow win = (CustomWindow) iw;
    			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();
    			// Disable buttons until the training has finished
    			win.setButtonsEnabled(false);

    			win.setTrainingComplete(false);

    			if( wekaSegmentation.trainClassifier() )
    			{
    				win.setTrainingComplete(true);
    				/*
    				wekaSegmentation.applyClassifier(false, null);
    				win.setClassfiedImage( wekaSegmentation.getClassifiedImage() );
    				if(win.isToogleEnabled())
    					win.toggleOverlay();
    				win.toggleOverlay();
    				*/
    			}
    			win.updateButtonsEnabling();
    		}
    	}

    	/**
    	 * Get the current result (labeled image)
    	 */
    	public static void getResult()
    	{
    		/*
    		final ImageWindow iw = WindowManager.getCurrentImage().getWindow();
    		if( iw instanceof CustomWindow )
    		{
    			final CustomWindow win = (CustomWindow) iw;
    			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();

    			ImagePlus classifiedImage =  wekaSegmentation.getClassifiedImage();
    			if( null == classifiedImage )
    			{
    				// check if the training is complete
    				if( win.trainingComplete )
    				{
    					// if not result image is there yet, calculate it
    					win.setButtonsEnabled( false );
    					wekaSegmentation.applyClassifier( false );
    					classifiedImage = wekaSegmentation.getClassifiedImage();
    					win.updateButtonsEnabling();
    				}
    				else
    				{
    					IJ.log( "Result image could not be created: "
    							+ " you need to train or load a classifier first." );
    					return;
    				}
    			}
    			final ImagePlus resultImage = classifiedImage.duplicate();

    			resultImage.setTitle("Classified image");

    			convertTo8bitNoScaling( resultImage );

    			resultImage.getProcessor().setColorModel( win.getOverlayLUT() );
    			resultImage.getImageStack().setColorModel( win.getOverlayLUT() );
    			resultImage.updateAndDraw();

    			resultImage.show();
    		}
    		*/
    	}

    	/**
    	 * Display the current probability maps
    	 */
    	public static void getProbability()
    	{
    		IJ.log("not implemented");
    		/*
    		final ImageWindow iw = WindowManager.getCurrentImage().getWindow();
    		if( iw instanceof CustomWindow )
    		{
    			final CustomWindow win = (CustomWindow) iw;
    			final WekaSegmentation wekaSegmentation = win.getWekaSegmentation();

    			IJ.showStatus("Calculating probability maps...");
    			IJ.log("Calculating probability maps...");
    			win.setButtonsEnabled(false);
    			wekaSegmentation.applyClassifier(true);
    			final ImagePlus probImage = wekaSegmentation.getClassifiedImage();
    			if(null != probImage)
    			{
    				probImage.setOpenAsHyperStack( true );
    				probImage.show();
    			}
    			win.updateButtonsEnabling();
    			IJ.showStatus("Done.");
    			IJ.log("Done");
    		}*/
    	}
	/**
	 * Display the whole image after classification
	 */
	void showClassificationImage()
	{
		// TODO: implement
		/*
		if( null == classifiedImage )
		{
			// if not result image is there yet, calculate it
			win.setButtonsEnabled( false );
			wekaSegmentation.applyClassifier( false );
			classifiedImage = wekaSegmentation.getClassifiedImage();
			win.updateButtonsEnabling();
		}

		final ImagePlus resultImage = classifiedImage.duplicate();

		resultImage.setTitle( "Classified image" );

		convertTo8bitNoScaling( resultImage );

		resultImage.getProcessor().setColorModel( overlayLUT );
		resultImage.getImageStack().setColorModel( overlayLUT );
		resultImage.updateAndDraw();

		resultImage.show();
		*/
	}

		/**
    	 * Display the current probability maps
    	 */
    	void showProbabilityImage()
    	{
    		// TODO: implement
    		/*
    		IJ.showStatus("Calculating probability maps...");
    		IJ.log("Calculating probability maps...");
    		win.setButtonsEnabled(false);
    		try{
    			wekaSegmentation.applyClassifier(true);
    		}catch(Exception ex){
    			IJ.log("Error while applying classifier! (please send bug report)");
    			ex.printStackTrace();
    			win.updateButtonsEnabling();
    			return;
    		}
    		final ImagePlus probImage = wekaSegmentation.getClassifiedImage();
    		if(null != probImage)
    		{
    			probImage.setDimensions( numOfClasses, displayImage.getNSlices(),
    					displayImage.getNFrames());
    			if ( displayImage.getNSlices() * displayImage.getNFrames() > 1 )
    				probImage.setOpenAsHyperStack( true );
    			probImage.show();
    		}
    		win.updateButtonsEnabling();
    		IJ.showStatus("Done.");
    		IJ.log("Done");
    		*/
    	}


	public synchronized void setNumFeaturesPerResolution(Example example)
	{

		numFeaturesPerResolution = new ArrayList<>();

		for ( int i = 0; i <= example.maximumFeatureScale; i++)
		{
			numFeaturesPerResolution.add(0);
		}

		for ( String featureName : example.featureNames )
		{
			for ( int l = 0; l <= example.maximumFeatureScale; l++ )
			{
				if (featureName.contains("L" + l))
				{
					numFeaturesPerResolution.set(l, numFeaturesPerResolution.get(l) + 1);
					break;
				}
			}
		}
	}



	/**
	 * Select attributes of current data by BestFirst search.
	 * The data is reduced to the selected attributes (features).
	 *
	 * @return false if the current dataset is empty
	 */
	public boolean selectAttributes()
	{
		if(null == loadedTrainingData)
		{
			IJ.error("There is no data so select attributes from.");
			return false;
		}
		// Select attributes by BestFirst
		loadedTrainingData = selectAttributes(loadedTrainingData);
		// Update list of features to use
		this.featureNames = new ArrayList<String>();
		IJ.log("Selected attributes:");
		for(int i = 0; i < loadedTrainingData.numAttributes(); i++)
		{
			this.featureNames.add(loadedTrainingData.attribute(i).name());
			IJ.log((i+1) + ": " + this.featureNames.get(i));
		}

		return true;
	}

	/**
	 * Select attributes using BestFirst search to reduce
	 * the number of parameters per instance of a dataset
	 *
	 * @param data input set of instances
	 * @return resampled set of instances
	 */
	public static Instances selectAttributes(Instances data)
	{
		final AttributeSelection filter = new AttributeSelection();
		Instances filteredIns = null;
		// Evaluator
		final CfsSubsetEval evaluator = new CfsSubsetEval();
		evaluator.setMissingSeparate(true);
		// Assign evaluator to filter
		filter.setEvaluator(evaluator);
		// Search strategy: best first (default values)
		final BestFirst search = new BestFirst();
		filter.setSearch(search);
		// Apply filter
		try {
			filter.setInputFormat(data);

			filteredIns = Filter.useFilter(data, filter);
		} catch (Exception e) {
			IJ.log("Error when resampling input data with selected attributes!");
			e.printStackTrace();
		}
		return filteredIns;

	}


    public void setInterpolatedFeatureSlice(int z, double[][][] featureSlice)
    {
        final int[] borderSizes = wekaSegmentation.getFeatureBorderSizes();
        int xs = borderSizes[0];
        int xe = multiResolutionFeatureImageArray.get(0).getWidth() - borderSizes[0] - 1;
        int ys = borderSizes[1];
        int ye = multiResolutionFeatureImageArray.get(0).getHeight() - borderSizes[1] - 1;

        int nf = getNumFeatures();

        for ( int f = 0; f < nf; f++ )
        {
            ImagePlus imp = multiResolutionFeatureImageArray[ f ];
            Calibration calibration = imp.getCalibration();
            double xCal = calibration.pixelWidth;
            double yCal = calibration.pixelHeight;
            double zCal = calibration.pixelDepth;
            double xHalfWidth = (xCal - 1) / 2 ;
            double yHalfWidth = (yCal - 1) / 2 ;
            double zHalfWidth = (zCal - 1) / 2 ;

            double v000,v100,vA00,v010,v110,vA10,v001,v101,vA01,v011,v111,vA11;
            double vAA0,vAA1,vAAA;
            double xTmp, xBaseDist, xBaseDist2;
            int xBase, yBase, yBaseOffset, yAboveOffset, xAbove;

            int nxFeatureImage = imp.getWidth();

            // get feature values as doubles
            float[] pixelsBase = null;
            float[] pixelsAbove = null;

            double zTmp = ( (z - zHalfWidth) / zCal );
            int zBase = (int) zTmp;
            double zBaseDist = ( zTmp - zBase ) ;
            double zBaseDist2 = 1 - zBaseDist ;

            if (imp.getBitDepth() == 8)
            {
                pixelsBase = getBytesAsFloats((byte[]) (imp.getStack().getProcessor(zBase + 1).getPixels()));
                if ( zBaseDist > 0 )
                    pixelsAbove = getBytesAsFloats((byte[]) (imp.getStack().getProcessor(zBase + 1 + 1).getPixels()));
            }
            else if (imp.getBitDepth() == 16)
            {
                pixelsBase = getShortsAsFloats((short[]) (imp.getStack().getProcessor(zBase + 1).getPixels()));
                if ( zBaseDist > 0 )
                    pixelsAbove = getShortsAsFloats((short[]) (imp.getStack().getProcessor(zBase + 1 + 1).getPixels()));
            }
            else if (imp.getBitDepth() == 32)
            {
                pixelsBase = (float[]) (imp.getStack().getProcessor(zBase + 1).getPixels());
                if ( zBaseDist > 0 )
                    pixelsAbove = (float[]) (imp.getStack().getProcessor(zBase + 1 + 1).getPixels());
            }

            for (int y = ys; y <= ye; ++y)
            {

                double yTmp = (y - yHalfWidth ) / yCal;
                yBase = (int) yTmp;
                double yBaseDist = ( yTmp - yBase );
                double yBaseDist2 = 1 - yBaseDist;

                yBaseOffset = yBase * nxFeatureImage;
                yAboveOffset = yBaseOffset + nxFeatureImage;

                for (int x = xs; x <= xe; ++x)
                {
                    xTmp = (x - xHalfWidth ) / xCal ;
                    xBase = (int) xTmp;
                    xBaseDist = ( xTmp - xBase );
                    xBaseDist2 = ( 1 - xBaseDist );
                    xAbove = xBase + 1;

                    // x,y average plane 0
                    v000 = pixelsBase[yBaseOffset + xBase];
                    v100 = pixelsBase[yBaseOffset + xAbove];
                    vA00 = xBaseDist2 * v000 + xBaseDist * v100;

                    v010 = pixelsBase[yAboveOffset + xBase];
                    v110 = pixelsBase[yAboveOffset + xAbove];
                    vA10 = xBaseDist2 * v010 + xBaseDist * v110;

                    vAA0 = yBaseDist2 * vA00 + yBaseDist * vA10;

                    if ( zBaseDist > 0 )
                    {
                        // x,y average plane 1

                        v001 = pixelsAbove[yBaseOffset + xBase];
                        v101 = pixelsAbove[yBaseOffset + xAbove];
                        vA01 = xBaseDist2 * v001 + xBaseDist * v101;

                        v011 = pixelsAbove[yAboveOffset + xBase];
                        v111 = pixelsAbove[yAboveOffset + xAbove];
                        vA11 = xBaseDist2 * v011 + xBaseDist * v111;

                        vAA1 = yBaseDist2 * vA01 + yBaseDist * vA11;
                    }
                    else
                    {
                        vAA1 = 0;
                    }

                    // average plane 0 and 1
                    vAAA = zBaseDist2 * vAA0 + zBaseDist * vAA1;

                    featureSlice[x][y][f] = vAAA;

                }
            }
        }
    }



	/**
	 * Classify instances concurrently
	 *
	 * @param data set of instances to classify
	 * @param classifier current classifier
	 * @param counter auxiliary counter to be able to update the progress bar
	 * @param probabilityMaps return a probability map for each class instead of a
	 * classified image
	 * @return classification result
	 */
	private static Callable<double[][]> classifyInstances(
			final Instances data,
			final AbstractClassifier classifier,
			final AtomicInteger counter,
			final boolean probabilityMaps)
	{
		if (Thread.currentThread().isInterrupted())
			return null;

		return new Callable<double[][]>(){

			@Override
			public double[][] call(){

				final int numInstances = data.numInstances();
				final int numClasses   = data.numClasses();

				final double[][] classificationResult;

				if (probabilityMaps)
					classificationResult = new double[numClasses][numInstances];
				else
					classificationResult = new double[1][numInstances];

				for (int i=0; i<numInstances; i++)
				{
					try{

						if (0 == i % 4000)
						{
							if (Thread.currentThread().isInterrupted())
								return null;
							counter.addAndGet(4000);
						}

						if (probabilityMaps)
						{
							double[] prob = classifier.distributionForInstance(data.get(i));
							for(int k = 0 ; k < numClasses; k++)
								classificationResult[k][i] = prob[k];
						}
						else
						{
							classificationResult[0][i] = classifier.classifyInstance(data.get(i));
						}

					}catch(Exception e){

						IJ.showMessage("Could not apply Classifier!");
						e.printStackTrace();
						return null;
					}
				}
				return classificationResult;
			}
		};
	}


	private double[] getMultiColorFeatureValues(
			double[][][][] featureSlice,
			int x,
			int y,
			int classNum)
	{
		int nFeaturesPerChannel = featureSlice[0][0][0].length;
		int nFeatures =  nFeaturesPerChannel * 2;
		nFeatures = classNum > -1 ? nFeatures + 1 : nFeatures;
		double[] featureValues = new double[ nFeatures ];

		for ( int c = 0; c < activeChannels.size(); c++ )
		{
			System.arraycopy( featureSlice[c][x][y], 0, featureValues, c * nFeaturesPerChannel, nFeaturesPerChannel );
		}

		if( classNum > -1 )
		{
			// set class value
			featureValues[ featureValues.length ] = classNum;
		}

		return ( featureValues );

	}

		private double[] getFeatureValuesOld(
    			double[][][][] featureValues,
    			int x,
    			int y,
    			int classNum,
    			boolean onlyActiveFeatures)
    	{
    		int numActiveFeatures = getNumActiveFeatures();
    		if ( classNum > -1 ) numActiveFeatures++;
    		double[] values = new double[ numActiveFeatures ];

    		int numFeaturesPerChannel = featureValues[0][0][0].length;
    		int iFeature = 0;
    		int iActiveFeature = 0;
    		for ( int c = 0; c < activeChannels.size(); c++ )
    		{
    			for (int i = 0; i < numFeaturesPerChannel; i++)
    			{
    				if ( onlyActiveFeatures )
    				{
    					if ( featureListSubset.get(iFeature++).isActive )
    					{
    						values[iActiveFeature++] = featureValues[c][x][y][i];
    					}
    				}
    				else
    				{
    					values[iActiveFeature++] = featureValues[c][x][y][i];
    				}

    			}
    		}

    		if( classNum > -1 ) values[iActiveFeature] = classNum;

    		return ( values );
    	}


    		private double[] getBalancedFeatureValues(
        			double[][][][] featureValues,
        			int x,
        			int y,
        			int classNum)
        	{
        		int nResolutions = numFeaturesPerResolution.size();
        		int nBalancedFeatures = getNumBalancedFeatureValues();
        		nBalancedFeatures = classNum > -1 ? nBalancedFeatures + 1 : nBalancedFeatures;
        		double[] balancedFeatureValues = new double[ nBalancedFeatures ];

        		int iBalanced = 0;
        		for ( int c = 0; c < activeChannels.size(); c++ )
        		{
        			int iFeatureThisChannel = 0;
        			for ( int r = 0; r < nResolutions; r++ ) // resolutions
        			{
        				int n = numFeaturesPerResolution.get(r);
        				int w = resolutionWeights.get(r);
        				for (int i = 0; i < n; i++) // all features in resolution
        				{
        					for (int j = 0; j < w; j++) // set this feature w times
        					{
        						balancedFeatureValues[iBalanced++] = featureValues[c][x][y][iFeatureThisChannel];
        					}
        					iFeatureThisChannel++;
        				}
        			}
        		}

        		if( classNum > -1 )
        		{
        			// set class value
        			balancedFeatureValues[iBalanced] = classNum;
        		}

        		return ( balancedFeatureValues );

        	}



	// TODO:
	// - the idea of the balancing is currently not used...
	// maybe remove this completely again? It should not matter too much
	// anymore due to the high fraction of random features
	public int getNumBalancedFeatureValues()
	{
		int nResolutions = numFeaturesPerResolution.size();
		int nC = activeChannels.size();
		int nBalancedFeatures = 0;
		for ( int iResolution = 0; iResolution < nResolutions; iResolution++ )
		{
			nBalancedFeatures += nC * numFeaturesPerResolution.get(iResolution)
					* resolutionWeights.get(iResolution);
		}
		return( nBalancedFeatures );


	}



	/**
	 * @param region5D
	 * @param sizesMinMax
	 * @return
	 */
	public Runnable postProcess(final Region5D region5D, int[] sizesMinMax)
	{
		return () ->
		{
			if (stopCurrentThreads || Thread.currentThread().isInterrupted())
			{
				//logger.info("Thread "+counter+"/"+counterMax+" stopped.");
				return;
			}

			/*
			logger.info("Classifying region "+counter+"/"+counterMax+" at "
							+ region5DToClassify.offset.getX() + ","
							+ region5DToClassify.offset.getY() + ","
							+ region5DToClassify.offset.getZ() + "..."
			);*/
			//logger.info("Memory usage [MB]: " + IJ.currentMemory() / 1000000L + "/" + IJ.maxMemory() / 1000000L);

			long start = System.currentTimeMillis();

			ImagePlus imp = null;

			if (region5D != null)
			{
				imp = Utils.getDataCube(inputImage, region5D, new int[]{-1, -1}, 1);
			}
			else
			{
				imp = classifiedImage; // whole image
			}

			// remove small objects
			trainableDeepSegmentation.utils.Utils.filterSmallObjects3D(imp, sizesMinMax);

			// save changed classification image
			ExecutorService exe = Executors.newFixedThreadPool(threadsPerRegion);
			ArrayList<Future> futures = new ArrayList<>();
			for (int z = 0; z < imp.getNSlices(); ++z)
			{
				Region5D region5DThisSlice = new Region5D();
				region5DThisSlice.size = new Point3D(imp.getWidth(), imp.getHeight(), 1);
				region5DThisSlice.offset = new Point3D(
						region5D.offset.getX(),
						region5D.offset.getY(),
						region5D.offset.getZ() + z);
				region5DThisSlice.t = region5D.t;
				region5DThisSlice.c = 0;
				region5DThisSlice.subSampling = new Point3D(1, 1, 1);
				byte[] thisSlice = (byte[]) imp.getStack().getProcessor(z + 1).getPixels();
				futures.add(
						exe.submit(
								setClassificationResult(
										classifiedImage,
										region5DThisSlice,
										thisSlice
								)
						)
				);
			}

			trainableDeepSegmentation.utils.Utils.joinThreads(futures, logger);
			exe.shutdown();
		};

	}




	public void postProcessSelectedRegion( String command, String zRange, String sizeRange )
	{

		if ( classifiedImage == null )
		{
			logger.error("classification_result image not set!");
			return;
		}

		if ( command.equals("STOP") )
		{
			logger.info("Stopping post-processing thread...");
			wekaSegmentation.stopCurrentThreads = true;
			postProcessButton.setText("Post process");
			return;
		}
		else
		{
			Roi roi = displayImage.getRoi();
			if (roi == null || !(roi.getType() == Roi.RECTANGLE))
			{
				IJ.showMessage("Please use ImageJ's rectangle selection tool" +
						" in order to select a region.");
				return;
			}

			postProcessButton.setText("STOP");
			wekaSegmentation.stopCurrentThreads = false;

			Thread thread = new Thread() {
				public void run()
				{

					int[] sizesMinMax = bigDataTools.utils.Utils.delimitedStringToIntegerArray(sizeRange,",");

					int zs, ze, zc;
					if ( zRange.equals("None") )
					{
						zc = displayImage.getZ();
						zs = zc - wekaSegmentation.getFeatureVoxelSizeAtMaximumScale();
						ze = zc + wekaSegmentation.getFeatureVoxelSizeAtMaximumScale();
					}
					else
					{
						int[] tmp = bigDataTools.utils.Utils.delimitedStringToIntegerArray(zRange,",");
						zs = tmp[0] - 1; ze = tmp[1] - 1;
						zc = ( ze + zs ) / 2;
					}

					Rectangle rectangle = roi.getBounds();
					Region5D region5D = new Region5D();
					region5D.t = displayImage.getT() - 1;
					region5D.c = displayImage.getC() - 1;
					region5D.size = new Point3D( rectangle.width, rectangle.height, ze - zs + 1);
					region5D.subSampling = new Point3D(1, 1, 1);
					region5D.offset = new Point3D( rectangle.x,  rectangle.y, zs );

					wekaSegmentation.postProcess(region5D, sizesMinMax).run();

					if (showColorOverlay)
						win.toggleOverlay();
					win.toggleOverlay();

					// we're done
					postProcessButton.setText("Post process");
				}
			};
			thread.start();
		}
	}


/*
								CODE FOR DEBUGGING

								int xGlobal = x + (int)region5D.offset.getX() + (int)region5Dglobal.offset.getX();
								int yGlobal = y + (int)region5D.offset.getY() + (int)region5Dglobal.offset.getY();
								int zGlobal = z + (int)region5D.offset.getZ() + (int)region5Dglobal.offset.getZ();

								if ( (xGlobal == 1838) & (yGlobal == 716) & (zGlobal == 588) )
								{
									IJ.log("global offset " + region5Dglobal.offset);
									IJ.log("local offset " + region5D.offset);
									IJ.log("x,y,z global: " + xGlobal + "," + yGlobal + "," + zGlobal);

									//featureImages.getMultiResolutionFeatureImageArray()[0][0].show();

									for (int f = 0; f < nf; f++)
									{
										if (f < 7)
											valueString[0] = valueString[0] + f + ":" + (featureSlice[x][y][f]) + ", ";
										else if (f < 7 + 24)
											valueString[1] = valueString[1] + f + ":" +(int) Math.round(featureSlice[x][y][f]) + ", ";
										else if (f < 7 + 24 + 27)
											valueString[2] = valueString[2] + f + ":" +(int) Math.round(featureSlice[x][y][f]) + ", ";
										else
											valueString[3] = valueString[3] + f + ":" +(int) Math.round(featureSlice[x][y][f]) + ", ";
									}

									for (String s : valueString)
										IJ.log(" x,y global: " + xGlobal + "," + yGlobal + "," + zGlobal + " values " + s);

									IJ.log("Classification "+ classificationResult[z][iInstanceThisSlice-1] );

								}*/



private static Runnable setClassificationResult(
			ImagePlus classifiedImage,
			FinalInterval singleSlice,
			byte[] classifiedSlice
	)
	{

		if (Thread.currentThread().isInterrupted())
			return null;

		return () -> {

			if ( classifiedImage.getStack() instanceof VirtualStackOfStacks )
			{
				VirtualStackOfStacks classifiedImageStack = (VirtualStackOfStacks) classifiedImage.getStack();
				try
				{
					Region5D region5D = ImageUtils.convertIntervalToRegion5D( singleSlice );
					classifiedImageStack.setAndSaveBytePixels( classifiedSlice, region5D );
				}
				catch (IOException e)
				{
					logger.warning("WekaSegmentation.setSliceInterval: " + e.toString());
				}
			}
			else
			{
				int z = (int)singleSlice.min( ImageUtils.Z );
				int t = (int)singleSlice.min( ImageUtils.T );

				int n = classifiedImage.getStackIndex( 1, z+1, t+1 );
				ImageProcessor ip = classifiedImage.getStack().getProcessor( n );

				int i = 0;
				for ( int y = (int) singleSlice.min( ImageUtils.Y ); y <= singleSlice.max( ImageUtils.Y ); ++y )
					for ( int x = (int) singleSlice.min( ImageUtils.X ); x <= singleSlice.max( ImageUtils.X ); ++x )
						ip.set(x, y, classifiedSlice[i++]);
			}

		};

	}



	private void storeClassificationResults( int numThreads,
											 ArrayList< byte [] > classificationResults,
											 FinalInterval tileInterval,
											 boolean isLogging )
	{

		long startTime = System.currentTimeMillis();

		exe = Executors.newFixedThreadPool( numThreads );
		ArrayList<Future> savingFutures = new ArrayList<>();

		for( int iSlice = 0; iSlice < classificationResults.size(); ++iSlice )
		{
			long z = iSlice + tileInterval.min( ImageUtils.Z );
			FinalInterval sliceInterval = IntervalUtils.fixDimension( tileInterval, ImageUtils.Z, z);

			savingFutures.add(
					exe.submit(
							setClassificationResult(
									classifiedImage,
									sliceInterval,
									classificationResults.get(iSlice)
							)
					)
			);
		}

		ThreadUtils.joinThreads(savingFutures,logger );
		exe.shutdown();

		if( isLogging )
		{
			logger.info("Saved classification results in [ms]: " + (System.currentTimeMillis() - startTime) );
		}

	}


            /*
            imp = multiResolutionFeatureImageArray.get(f);
            calibration = imp.getCalibration();
            xCal = calibration.pixelWidth;
            yCal = calibration.pixelHeight;
            zCal = calibration.pixelDepth;
            xHalfWidth = (xCal - 1) / 2 ;
            yHalfWidth = (yCal - 1) / 2 ;
            zHalfWidth = (zCal - 1) / 2 ;

            nxFeatureImage = imp.getWidth();

            /*

            # Example: binning 3, value 3

            orig: 0,1,2,3,4,5,6,7,8
            bin:  .,0,.|.,1,.|.,2,.
            value at orig 3 should be computed from bin 0 and 1
            halfwidth = (3-1)/2 = 1
            tmp = (3 - halfwidth) / 3 = 2 / 3
            base = (int) tmp = 0
            baseDist = tmp - base = 2 / 3 - 0 = 2 / 3
            baseDist2 = 1 - 2 / 3 = 1 / 3
            ..this makes sense, because
            3 is 2 away from 1, which is the center of bin 0
            3 is 1 away from 4, which is the center of bin 1

            # Example: binning 3, value 4

            orig: 0,1,2,3,4,5,6,7,8
            bin:  .,0,.|.,1,.|.,2,.
            value at orig 4 should be computed from bin 1, because it is its center
            halfwidth = (3-1)/2 = 1
            tmp = (4 - halfwidth) / 3 = 3 / 3 = 1
            base = (int) tmp = 1
            baseDist = tmp - base = 1 - 1 = 0
            baseDist2 = 1 - 0 = 1
            ..this means that bin 2 will get a weight of 0 (i.e., baseDist)
            ..and bin 1 will get a weight of 1

            # Example: binning=cal=2, value 3

            orig: 0,1,2,3,4,5,6,7,8
            bin:  .0.|.1.|.2.|.3.|
            value at orig 3 should be computed from bin 1 and bin 2
            halfwidth = (cal-1)/2 = (2-1)/2 = 0.5
            tmp = (3 - halfwidth) / cal = (3-0.5)/2 = 2.5/2 = 1.25
            base = (int) tmp = 1  (=> above will be 2)
            baseDist = tmp - base = 1.25 - 1 = 0.25
            baseDist2 = 1 - 0.25 = 0.75
            ..center of bin 1 is 2.5 in orig
            ..center of bin 2 is 4.5 in orig
            ..distance of 3 from 2.5 is 0.5
            ..distance of 3 from 4.5 is 1.5
            ..dividing 0.5 and 1.5 by the calibration (=binning) 2 yields the scaled distances 0.25 and 0.75
            ..=> makes sense
            */

            /*
            zTmp = ( (z - zHalfWidth) / zCal );
            zBase = (int) zTmp;
            zBaseDist = ( zTmp - zBase ) ;
            zBaseDist2 = 1 - zBaseDist ;

            if (imp.getBitDepth() == 8)
            {
                pixelsBase = getBytesAsFloats((byte[]) (imp.getStack().getProcessor(zBase + 1).getPixels()));
                if ( zBaseDist > 0 )
                    pixelsAbove = getBytesAsFloats((byte[]) (imp.getStack().getProcessor(zBase + 1 + 1).getPixels()));
            }
            else if (imp.getBitDepth() == 16)
            {
                pixelsBase = getShortsAsFloats((short[]) (imp.getStack().getProcessor(zBase + 1).getPixels()));
                if ( zBaseDist > 0 )
                    pixelsAbove = getShortsAsFloats((short[]) (imp.getStack().getProcessor(zBase + 1 + 1).getPixels()));
            }
            else if (imp.getBitDepth() == 32)
            {
                pixelsBase = (float[]) (imp.getStack().getProcessor(zBase + 1).getPixels());
                if ( zBaseDist > 0 )
                    pixelsAbove = (float[]) (imp.getStack().getProcessor(zBase + 1 + 1).getPixels());
            }

            for (y = ys; y <= ye; ++y)
            {
                yTmp = (y - yHalfWidth ) / yCal;
                yBase = (int) yTmp;
                yBaseDist = (yTmp - yBase);
                yBaseDist2 = 1 - yBaseDist;

                yBaseOffset = yBase * nxFeatureImage;
                yAboveOffset = yBaseOffset + nxFeatureImage;

                for (x = xs; x <= xe; ++x)
                {
                    xTmp = (x - xHalfWidth ) / xCal ;
                    xBase = (int) xTmp;
                    xBaseDist = ( xTmp - xBase );
                    xBaseDist2 = ( 1 - xBaseDist );
                    xAbove = xBase + 1;

                    /*
                    v000 = pixelsBase[yBaseOffset + xBase];
                    v100 = pixelsBase[yBaseOffset + xAbove];
                    vA00 = xBaseDist2 * v000 + xBaseDist * v100;

                    v010 = pixelsBase[yAboveOffset + xBase];
                    v110 = pixelsBase[yAboveOffset + xAbove];
                    vA10 = xBaseDist2 * v010 + xBaseDist * v110;
                    */

                    vA00 = xBaseDist2 * pixelsBase[yBaseOffset + xBase] + xBaseDist * pixelsBase[yBaseOffset + xAbove];
                    vA10 = xBaseDist2 * pixelsBase[yAboveOffset + xBase] + xBaseDist * pixelsBase[yAboveOffset + xAbove];
                    vAA0 = yBaseDist2 * vA00 + yBaseDist * vA10;

                    if ( zBaseDist > 0 )
                    {
                        /*
                        v001 = pixelsAbove[yBaseOffset + xBase];
                        v101 = pixelsAbove[yBaseOffset + xAbove];
                        vA01 = xBaseDist2 * v001 + xBaseDist * v101;

                        v011 = pixelsAbove[yAboveOffset + xBase];
                        v111 = pixelsAbove[yAboveOffset + xAbove];
                        vA11 = xBaseDist2 * v011 + xBaseDist * v111;
                        */

                        vA01 = xBaseDist2 * pixelsAbove[yBaseOffset + xBase] + xBaseDist * pixelsAbove[yBaseOffset + xAbove];
                        vA11 = xBaseDist2 * pixelsAbove[yAboveOffset + xBase] + xBaseDist * pixelsAbove[yAboveOffset + xAbove];
                        vAA1 = yBaseDist2 * vA01 + yBaseDist * vA11;
                    }
                    else
                    {
                        vAA1 = 0;
                    }

                    vAAA = zBaseDist2 * vAA0 + zBaseDist * vAA1;

                    featureSlice[x-xs][y-ys][f] = vAAA;

                }
            }
